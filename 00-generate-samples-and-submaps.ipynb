{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import time\n",
    "import h5py\n",
    "import csv\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "\n",
    "from numpy import linalg as LA\n",
    "from scipy.spatial import Delaunay\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#sys.path.insert(0, \"../\")\n",
    "from info3d import *\n",
    "from nn_matchers import *\n",
    "\n",
    "from query_sets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "\n",
    "radius_range = np.arange(0.5,1.6,0.5)\n",
    "\n",
    "with open('point_collection/new_contiguous_point_collection.pickle','rb') as f: \n",
    "    new_contiguous_point_collection = pickle.load(f)\n",
    "\n",
    "point_collection_indices = np.arange(len(new_contiguous_point_collection))\n",
    "point_collection_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0.1: Getting sample points for one-time partial radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 1000\n",
    "\n",
    "sample_points = []\n",
    "\n",
    "samples_indeces = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in np.arange(samples):\n",
    "\n",
    "    random_object = np.random.choice(point_collection_indices)\n",
    "\n",
    "    object_name = new_contiguous_point_collection[random_object][0]\n",
    "    pointCloud = new_contiguous_point_collection[random_object][1]\n",
    "    triangles = new_contiguous_point_collection[random_object][2]\n",
    "    \n",
    "    triangle_index = np.random.choice(np.arange(len(triangles)))\n",
    "    vertex_index = triangles[triangle_index,1]\n",
    "    original_vertex = pointCloud[vertex_index]\n",
    "\n",
    "    sample_points.append([\n",
    "        random_object, \n",
    "        object_name, \n",
    "        original_vertex\n",
    "    ])\n",
    "    \n",
    "    samples_indeces.append(random_object)\n",
    "    \n",
    "print(\"Done generating\",len(sample_points),\"samples in {:.3f} seconds.\".format(time.time()-t0))\n",
    "\n",
    "with open('sample_points.pickle','wb') as f:\n",
    "    pickle.dump(sample_points,f)\n",
    "    \n",
    "plt.title(\"Distribution of the sample spaces\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Sample space\")\n",
    "plt.hist(samples_indeces,bins = np.arange(0,8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0.2: Creating a synthetic set of successive partial spaces\n",
    "\n",
    "Similar to the partial case above, we use the same sample points, i.e. centroids, for successive releases but will only vary the size of the partial space for every release.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples = 100\n",
    "releases = 100\n",
    "\n",
    "nearby_range = 2.0\n",
    "    \n",
    "t1 = time.time()\n",
    "\n",
    "successive_sample_points = []\n",
    "\n",
    "for i in np.arange(samples):#\n",
    "\n",
    "    random_object = np.random.choice(point_collection_indices)\n",
    "    #reference_ransac = np.random.randint(5)\n",
    "\n",
    "    object_name = new_contiguous_point_collection[random_object][0]\n",
    "    pointCloud = new_contiguous_point_collection[random_object][1]\n",
    "    triangles = new_contiguous_point_collection[random_object][2]\n",
    "\n",
    "    current_vertex = pointCloud[np.random.randint(len(pointCloud))]\n",
    "\n",
    "    growing_point_collection_vertices = [[\n",
    "        random_object, \n",
    "        object_name, \n",
    "        current_vertex\n",
    "    ]]\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(20000,len(pointCloud)),algorithm='kd_tree').fit(pointCloud[:,:3])\n",
    "\n",
    "    for release in np.arange(releases-1):\n",
    "\n",
    "        distances, indices = nbrs.kneighbors([current_vertex[:3]])\n",
    "\n",
    "        cand_indices = indices[0,np.where(distances[0]<(nearby_range))[0]]\n",
    "\n",
    "        distribution = np.sort(abs(np.random.normal(nearby_range*0.5,nearby_range*0.3,len(cand_indices))))\n",
    "\n",
    "        current_vertex = pointCloud[\n",
    "            np.random.choice(\n",
    "                cand_indices,\n",
    "                p = distribution/np.sum(distribution)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        growing_point_collection_vertices.append([\n",
    "            random_object, \n",
    "            object_name, \n",
    "            current_vertex\n",
    "        ])\n",
    "\n",
    "    successive_sample_points.append([\n",
    "        [random_object, object_name],\n",
    "        growing_point_collection_vertices\n",
    "    ])\n",
    "\n",
    "    if i % 33 == 1:\n",
    "        print(\" Done with successive {} sample_points extraction in {:.3f} seconds\".format(i,time.time()-t1))\n",
    "        t1 = time.time()\n",
    "\n",
    "    with open('successive_sample_points.pickle','wb') as f:\n",
    "        pickle.dump(successive_sample_points,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "\n",
    "try:\n",
    "    with open('successive_sample_points.pickle','rb') as f:\n",
    "        successive_point_collection = pickle.load(f)\n",
    "\n",
    "    samples = len(successive_point_collection)\n",
    "    releases = len(successive_point_collection[0][1])\n",
    "\n",
    "    print(samples,\"samples for radius\",radius)\n",
    "    print(releases,\"releases each\")\n",
    "\n",
    "except Exception as e1:\n",
    "    print(e1)\n",
    "\n",
    "successive_sample_points_per_release = [[]]\n",
    "\n",
    "for k, [obj_, growing_point_collection] in enumerate(successive_point_collection):\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    successive_sample_points = []\n",
    "\n",
    "    reference_ransac = np.random.randint(5)\n",
    "\n",
    "    for i, obj_meta in enumerate(growing_point_collection):\n",
    "\n",
    "        successive_sample_points.append([obj_meta, reference_ransac])\n",
    "\n",
    "        try:\n",
    "            successive_sample_points_per_release[i].append(successive_sample_points)\n",
    "        except:\n",
    "            successive_sample_points_per_release.append([successive_sample_points])\n",
    "\n",
    "        #print(len(successive_sample_points_per_release[i]),len(successive_sample_points_per_release[i][k]))\n",
    "\n",
    "    with open('successive_sample_points_per_release.pickle','wb') as f:\n",
    "        pickle.dump(successive_sample_points_per_release,f)\n",
    "\n",
    "print(\" Done with successive sample_points extraction in {:.3f} seconds\".format(time.time()-t1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0.3: Create submaps for pointnetvlad using same samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_span = 2.0\n",
    "\n",
    "interval = 0.5\n",
    "\n",
    "num_points = 4096\n",
    "\n",
    "cutoff = 0.5\n",
    "\n",
    "with open('sample_points.pickle','rb') as f:\n",
    "    sample_points = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0.3.1: Generate the reference dataset using the raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_path = 'pointnetvlad_submaps/'\n",
    "\n",
    "if not os.path.exists(baseline_path): os.mkdir(baseline_path)\n",
    "\n",
    "raw_path = os.path.join(baseline_path,\"raw_dataset\")\n",
    "raw_pc_path = os.path.join(raw_path,\"pointcloud_4m_0.25\")\n",
    "\n",
    "if not os.path.exists(raw_path): os.mkdir(raw_path)\n",
    "if not os.path.exists(raw_pc_path): os.mkdir(raw_pc_path)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "csvfile = open(raw_path+\"/pointcloud_centroids_4m_0.25.csv\",'w',newline = '')\n",
    "\n",
    "csv_writer = csv.writer(csvfile, delimiter = ',')\n",
    "csv_writer.writerow(['timestamp', 'northing', 'easting','alting','obj'])\n",
    "\n",
    "for obj_, [object_name, pointCloud, triangles] in enumerate(new_contiguous_point_collection):\n",
    "\n",
    "    if object_name == \"Reception-Data61-L5.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) + 50\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 0\n",
    "    elif object_name == \"Driveway.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) - 25\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) - 50\n",
    "    elif object_name == \"Apartment.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) + 25\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) - 50\n",
    "    elif object_name == \"Workstations-Data61-L4.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) - 50\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 0\n",
    "    elif object_name == \"Kitchen-Data61-L4.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) + 0\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 0\n",
    "    elif object_name == \"HallWayToKitchen-Data61-L4.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) - 25\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 50\n",
    "    elif object_name == \"StairWell-Data61-L4.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) + 25\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 50\n",
    "\n",
    "    new_Y = pointCloud[:,1]\n",
    "        \n",
    "    new_object_pointcloud = np.stack((new_X,new_Z,new_Y)).T\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=20000, algorithm='kd_tree').fit(new_object_pointcloud)\n",
    "\n",
    "    round_new_pointcloud = 0.25*100*np.around((0.01/0.25)*new_object_pointcloud,decimals=2)\n",
    "    unq_round_pointcloud = np.unique(round_new_pointcloud[:,:3],axis = 0)\n",
    "\n",
    "    raw_centroids = unq_round_pointcloud#+np.random.normal(0,0.25,unq_round_pointcloud.shape)\n",
    "\n",
    "    for northing, easting, alting in raw_centroids:\n",
    "\n",
    "        # Getting the points around our centroid defined by [northing, easting]\n",
    "        distances, indices = nbrs.kneighbors([[northing, easting, alting]])\n",
    "\n",
    "        #if max(distances[0]) < 0.5*spatial_span: continue\n",
    "\n",
    "        submap_pointcloud = new_object_pointcloud[indices[0,np.where(distances[0,:]<=spatial_span)[0]]]\n",
    "\n",
    "        if len(submap_pointcloud) == 0:\n",
    "            continue\n",
    "\n",
    "        # Centering and rescaling\n",
    "        submap_pointcloud = (submap_pointcloud - [northing, easting, alting])/spatial_span\n",
    "\n",
    "        if len(submap_pointcloud) > num_points:\n",
    "            submap_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points)]\n",
    "        elif len(submap_pointcloud) < num_points and len(submap_pointcloud) >= cutoff*num_points :\n",
    "            #print(i,submap_pointcloud.shape)\n",
    "            additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud))]\n",
    "            additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "            submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "        elif len(submap_pointcloud) < cutoff*num_points :\n",
    "            #continue\n",
    "            #print(i,submap_pointcloud.shape)\n",
    "            additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud), True)]\n",
    "            additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "            submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "\n",
    "        timestamp = int(10**16*(time.time()))\n",
    "\n",
    "        csv_writer.writerow([timestamp,northing,easting,alting,obj_])\n",
    "\n",
    "        with open(raw_pc_path+'/{}.pickle'.format(timestamp),'wb') as f:\n",
    "            pickle.dump(submap_pointcloud.T,f)\n",
    "\n",
    "    print(\"Done with submap generation for object ({}) {} in {:.3f} seconds\".format(obj_,object_name,time.time()-t0))\n",
    "\n",
    "csvfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0.3.2: Generate a reference dataset using a sample RANSAC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to create the reference dataset using the raw dataset and a (randomly chosen) ransac dataset.\n",
    "\n",
    "num_points = 4096\n",
    "\n",
    "baseline_path = 'pointnetvlad_submaps/'\n",
    "\n",
    "if not os.path.exists(baseline_path): os.mkdir(baseline_path)\n",
    "    \n",
    "try:\n",
    "    trial = np.random.randint(5)\n",
    "\n",
    "    with open(\"../ransac_pc/ransac_point_collection_{}.pickle\".format(trial),'rb') as f:\n",
    "        ransac_trial_point_collection = pickle.load(f)\n",
    "\n",
    "    object_, pointCloud_, tri_ = ransac_trial_point_collection[0]\n",
    "    \n",
    "    print(\"Chosen ransac trial\",trial)\n",
    "    \n",
    "except Exception as ex:\n",
    "    print(\"Error:\",ex)\n",
    "\n",
    "raw_path = os.path.join(baseline_path,\"ransac_dataset\")\n",
    "raw_pc_path = os.path.join(raw_path,\"pointcloud_4m_0.25\")\n",
    "\n",
    "if not os.path.exists(raw_path): os.mkdir(raw_path)\n",
    "if not os.path.exists(raw_pc_path): os.mkdir(raw_pc_path)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "csvfile = open(raw_path+\"/pointcloud_centroids_4m_0.25.csv\",'w',newline = '')\n",
    "\n",
    "csv_writer = csv.writer(csvfile, delimiter = ',')\n",
    "csv_writer.writerow(['timestamp', 'northing', 'easting','alting','obj'])\n",
    "\n",
    "for obj_, [object_name, pointCloud, triangles] in enumerate(ransac_trial_point_collection):\n",
    "\n",
    "    if object_name == \"Reception-Data61-L5.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) + 50\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 0\n",
    "    elif object_name == \"Driveway.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) - 25\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) - 50\n",
    "    elif object_name == \"Apartment.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) + 25\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) - 50\n",
    "    elif object_name == \"Workstations-Data61-L4.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) - 50\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 0\n",
    "    elif object_name == \"Kitchen-Data61-L4.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) + 0\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 0\n",
    "    elif object_name == \"HallWayToKitchen-Data61-L4.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) - 25\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 50\n",
    "    elif object_name == \"StairWell-Data61-L4.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) + 25\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 50\n",
    "\n",
    "    new_Y = pointCloud[:,1]\n",
    "        \n",
    "    new_object_pointcloud = np.stack((new_X,new_Z,new_Y)).T\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=20000, algorithm='kd_tree').fit(new_object_pointcloud)\n",
    "\n",
    "    round_new_pointcloud = 0.25*100*np.around((0.01/0.25)*new_object_pointcloud,decimals=2)\n",
    "    unq_round_pointcloud = np.unique(round_new_pointcloud[:,:3],axis = 0)\n",
    "\n",
    "    raw_centroids = unq_round_pointcloud#+np.random.normal(0,0.25,unq_round_pointcloud.shape)\n",
    "\n",
    "    for northing, easting, alting in raw_centroids:\n",
    "\n",
    "        # Getting the points around our centroid defined by [northing, easting]\n",
    "        distances, indices = nbrs.kneighbors([[northing, easting, alting]])\n",
    "\n",
    "        #if max(distances[0]) < 0.5*spatial_span: continue\n",
    "\n",
    "        submap_pointcloud = new_object_pointcloud[indices[0,np.where(distances[0,:]<=spatial_span)[0]]]\n",
    "\n",
    "        if len(submap_pointcloud) == 0:\n",
    "            continue\n",
    "\n",
    "        # Centering and rescaling\n",
    "        submap_pointcloud = (submap_pointcloud - [northing, easting, alting])/spatial_span\n",
    "\n",
    "        if len(submap_pointcloud) > num_points:\n",
    "            submap_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points)]\n",
    "        elif len(submap_pointcloud) < num_points and len(submap_pointcloud) >= cutoff*num_points :\n",
    "            #print(i,submap_pointcloud.shape)\n",
    "            additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud))]\n",
    "            additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "            submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "        elif len(submap_pointcloud) < cutoff*num_points :\n",
    "            #continue\n",
    "            #print(i,submap_pointcloud.shape)\n",
    "            additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud), True)]\n",
    "            additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "            submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "\n",
    "        timestamp = int(10**16*(time.time()))\n",
    "\n",
    "\n",
    "        csv_writer.writerow([timestamp,northing,easting,alting,obj_])\n",
    "\n",
    "        with open(raw_pc_path+'/{}.pickle'.format(timestamp),'wb') as f:\n",
    "            pickle.dump(submap_pointcloud.T,f)\n",
    "\n",
    "    print(\"Done with submap generation for object ({}) {} in {:.3f} seconds\".format(obj_,object_name,time.time()-t0))\n",
    "\n",
    "csvfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0.3.3: Generate the test submaps:\n",
    " - using Raw spaces for validation\n",
    " - using Ransac spaces for evaluation\n",
    " - using Ransac spaces for the successive case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time releases Raw partial spaces\n",
    "\n",
    "baseline_path = 'pointnetvlad_submaps/'\n",
    "\n",
    "if not os.path.exists(baseline_path): os.mkdir(baseline_path)\n",
    "\n",
    "for radius in np.arange(0.25,3.1,0.25):\n",
    "    \n",
    "    per_radius_partial_length = []\n",
    "    \n",
    "    t1 = time.time()\n",
    "            \n",
    "    partial_path = os.path.join(baseline_path,\"raw_partial_radius_\"+str(radius)+\"_\"+str(num_points))+\"_unassisted\"\n",
    "    pointcloud_partial_path = os.path.join(partial_path,\"pointcloud_4m\")\n",
    "    #pointcloud_partial_bin_path = os.path.join(partial_path,\"pointcloud_4m_npy\")\n",
    "\n",
    "    if not os.path.exists(partial_path): os.mkdir(partial_path)\n",
    "    if not os.path.exists(pointcloud_partial_path): os.mkdir(pointcloud_partial_path)\n",
    "    #if not os.path.exists(pointcloud_partial_bin_path): os.mkdir(pointcloud_partial_bin_path)\n",
    "\n",
    "    print(\" \",pointcloud_partial_path)\n",
    "    #\"\"\"\n",
    "    csvfile = open(partial_path+\"/pointcloud_centroids_4m.csv\",'w',newline = '')\n",
    "\n",
    "    csv_writer = csv.writer(csvfile, delimiter = ',')\n",
    "    csv_writer.writerow(['timestamp', 'northing', 'easting', 'alting','obj'])\n",
    "    #\"\"\"\n",
    "    count = 0\n",
    "    \n",
    "    for obj_, object_name, original_vertex in sample_points:\n",
    "        \n",
    "        new_partial_pointcloud = []\n",
    "        new_vX = []\n",
    "        new_vZ = []\n",
    "        \n",
    "        try:\n",
    "\n",
    "            object_, ransac_pointCloud, tri_ = new_contiguous_point_collection[int(obj_)]\n",
    "            \n",
    "            ransac_nbrs = NearestNeighbors(n_neighbors=min(20000,len(ransac_pointCloud)), algorithm='kd_tree').fit(ransac_pointCloud[:,:3])\n",
    "            \n",
    "            dist_, ind_ = ransac_nbrs.kneighbors([original_vertex[:3]])\n",
    "            pointCloud =  ransac_pointCloud[ind_[0,np.where(dist_[0,:]<=radius)[0]]]\n",
    "        except:\n",
    "            print(\"Can't get ransac samples for\",trial,obj_meta[0],dist_.shape,ind_.shape)\n",
    "            continue\n",
    "            \n",
    "        #if len(gen_planes) == 0: continue\n",
    "        if len(pointCloud) == 0: continue\n",
    "\n",
    "        if object_name == \"Reception-Data61-L5.obj\":\n",
    "            new_X = pointCloud[:,0] + 50\n",
    "            new_Z = pointCloud[:,2] + 0\n",
    "            new_vX = original_vertex[0] + 50\n",
    "            new_vZ = original_vertex[2] + 0\n",
    "        elif object_name == \"Driveway.obj\":\n",
    "            new_X = pointCloud[:,0] - 25\n",
    "            new_Z = pointCloud[:,2] - 50\n",
    "            new_vX = original_vertex[0] - 25\n",
    "            new_vZ = original_vertex[2] - 50\n",
    "        elif object_name == \"Apartment.obj\":\n",
    "            new_X = pointCloud[:,0] + 25\n",
    "            new_Z = pointCloud[:,2] - 50\n",
    "            new_vX = original_vertex[0] + 25\n",
    "            new_vZ = original_vertex[2] - 50\n",
    "        elif object_name == \"Workstations-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] - 50\n",
    "            new_Z = pointCloud[:,2] + 0\n",
    "            new_vX = original_vertex[0] - 50\n",
    "            new_vZ = original_vertex[2] + 0\n",
    "        elif object_name == \"Kitchen-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] + 0\n",
    "            new_Z = pointCloud[:,2] + 0\n",
    "            new_vX = original_vertex[0] + 0\n",
    "            new_vZ = original_vertex[2] + 0\n",
    "        elif object_name == \"HallWayToKitchen-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] - 25\n",
    "            new_Z = pointCloud[:,2] + 50\n",
    "            new_vX = original_vertex[0] - 25\n",
    "            new_vZ = original_vertex[2] + 50\n",
    "        elif object_name == \"StairWell-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] + 25\n",
    "            new_Z = pointCloud[:,2] + 50\n",
    "            new_vX = original_vertex[0] + 25\n",
    "            new_vZ = original_vertex[2] + 50\n",
    "        else:\n",
    "            print(\"Error:\",obj_meta)\n",
    "\n",
    "        new_Y = pointCloud[:,1]\n",
    "\n",
    "        new_partial_pointcloud = np.stack((new_X,new_Z,new_Y)).T\n",
    "        \n",
    "        max_known_span = max(np.amax(new_partial_pointcloud, axis = 0) - np.amin(new_partial_pointcloud, axis = 0))\n",
    "        \n",
    "        nbrs = NearestNeighbors(n_neighbors=min(2*num_points,len(new_partial_pointcloud)), algorithm='kd_tree').fit(new_partial_pointcloud)\n",
    "\n",
    "        # Get submap \"centroids\" by quantizing by 0.25m, i.e. round then unique\n",
    "        if max_known_span > 3*spatial_span:\n",
    "            round_new_partial_pointcloud = 100*np.around(0.01*new_partial_pointcloud,decimals=2)\n",
    "            unq_round_partial_pointcloud = np.unique(round_new_partial_pointcloud[:,:3],axis = 0)\n",
    "        #    \n",
    "            raw_partial_centroids = unq_round_partial_pointcloud\n",
    "            c_nbrs = NearestNeighbors(n_neighbors = min(25,len(raw_partial_centroids)),  algorithm='kd_tree').fit(raw_partial_centroids)\n",
    "            c_dist, c_ind = c_nbrs.kneighbors(raw_partial_centroids)\n",
    "\n",
    "            ia1, ia2 = np.where(c_dist < 1.73)\n",
    "            \n",
    "\n",
    "            dist_bins = np.bincount(ia1)\n",
    "            max_dist = max(np.bincount(ia1))\n",
    "            raw_partial_centroids = raw_partial_centroids[[i for i, j in enumerate(dist_bins) if j == max_dist]]\n",
    "            \n",
    "            raw_partial_centroids = raw_partial_centroids+np.random.normal(0,interval,raw_partial_centroids.shape)\n",
    "            \n",
    "        else:\n",
    "            # Correcting this, because the attacker is supposed to not know the true centroid\n",
    "            # and has to estimate it instead.\n",
    "            #raw_partial_centroids = [[new_vX, new_vZ, original_vertex[1]]]        \n",
    "\n",
    "            raw_partial_centroids = [np.mean(new_partial_pointcloud, axis = 0)]\n",
    "            \n",
    "        for northing, easting, alting in raw_partial_centroids:\n",
    "            \n",
    "            # Getting the points around our centroid defined by [northing, easting]\n",
    "            distances, indices = nbrs.kneighbors([[northing, easting, alting]])\n",
    "            \n",
    "            #if max(distances[0]) < 0.5*spatial_span: continue\n",
    "                \n",
    "            submap_pointcloud = new_partial_pointcloud[indices[0,np.where(distances[0,:]<=spatial_span)[0]]]\n",
    "\n",
    "            if len(submap_pointcloud) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Centering and rescaling\n",
    "            submap_pointcloud = (submap_pointcloud - [northing, easting, alting])/spatial_span\n",
    "\n",
    "            if len(submap_pointcloud) > num_points:\n",
    "                submap_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points)]\n",
    "            elif len(submap_pointcloud) < num_points and len(submap_pointcloud) >= cutoff*num_points :\n",
    "                #print(i,submap_pointcloud.shape)\n",
    "                additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud))]\n",
    "                additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "                submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "            elif len(submap_pointcloud) < cutoff*num_points :\n",
    "                #continue\n",
    "                #print(i,submap_pointcloud.shape)\n",
    "                additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud), True)]\n",
    "                additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "                submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "\n",
    "            timestamp = int(10**16*(time.time()))\n",
    "            \n",
    "            csv_writer.writerow([timestamp,northing,easting,alting,obj_])\n",
    "            \n",
    "            with open(pointcloud_partial_path+'/{}.pickle'.format(timestamp),'wb') as f:\n",
    "                pickle.dump(submap_pointcloud.T,f)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "\n",
    "    print(\"   Done with submap generation for radius {} ( {} samples) in {:.3f} seconds\".format(radius,count,time.time()-t1))\n",
    "    csvfile.close()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time releases RANSAC partial spaces\n",
    "\n",
    "baseline_path = 'pointnetvlad_submaps/'\n",
    "\n",
    "if not os.path.exists(baseline_path): os.mkdir(baseline_path)\n",
    "\n",
    "for radius in np.arange(0.25,3.1,0.25):\n",
    "        \n",
    "    t1 = time.time()\n",
    "            \n",
    "    partial_path = os.path.join(baseline_path,\"ransac_partial_radius_\"+str(radius)+\"_\"+str(num_points))+\"_unassisted\"\n",
    "    pointcloud_partial_path = os.path.join(partial_path,\"pointcloud_4m\")\n",
    "    #pointcloud_partial_bin_path = os.path.join(partial_path,\"pointcloud_4m_npy\")\n",
    "\n",
    "    if not os.path.exists(partial_path): os.mkdir(partial_path)\n",
    "    if not os.path.exists(pointcloud_partial_path): os.mkdir(pointcloud_partial_path)\n",
    "    #if not os.path.exists(pointcloud_partial_bin_path): os.mkdir(pointcloud_partial_bin_path)\n",
    "\n",
    "    print(\" \",pointcloud_partial_path)\n",
    "    #\"\"\"\n",
    "    csvfile = open(partial_path+\"/pointcloud_centroids_4m.csv\",'w',newline = '')\n",
    "\n",
    "    csv_writer = csv.writer(csvfile, delimiter = ',')\n",
    "    csv_writer.writerow(['timestamp', 'northing', 'easting', 'alting','obj'])\n",
    "    #\"\"\"\n",
    "    count = 0\n",
    "    \n",
    "    for obj_, object_name, original_vertex in sample_points:\n",
    "        \n",
    "        new_partial_pointcloud = []\n",
    "        new_vX = []\n",
    "        new_vZ = []\n",
    "        \n",
    "        try:\n",
    "            trial = np.random.randint(5)\n",
    "            \n",
    "            with open(\"../ransac_pc/ransac_point_collection_{}.pickle\".format(trial),'rb') as f:\n",
    "                ransac_trial_point_collection = pickle.load(f)\n",
    "\n",
    "            object_, ransac_pointCloud, tri_ = ransac_trial_point_collection[int(obj_)]\n",
    "            \n",
    "            ransac_nbrs = NearestNeighbors(n_neighbors=min(20000,len(ransac_pointCloud)), algorithm='kd_tree').fit(ransac_pointCloud[:,:3])\n",
    "            \n",
    "            dist_, ind_ = ransac_nbrs.kneighbors([original_vertex[:3]])\n",
    "            pointCloud =  ransac_pointCloud[ind_[0,np.where(dist_[0,:]<=radius)[0]]]\n",
    "        except:\n",
    "            print(\"Can't get ransac samples for\",trial,obj_meta[0])\n",
    "            continue\n",
    "            \n",
    "        #if len(gen_planes) == 0: continue\n",
    "        if len(pointCloud) == 0: continue\n",
    "\n",
    "        if object_name == \"Reception-Data61-L5.obj\":\n",
    "            new_X = pointCloud[:,0] + 50\n",
    "            new_Z = pointCloud[:,2] + 0\n",
    "            new_vX = original_vertex[0] + 50\n",
    "            new_vZ = original_vertex[2] + 0\n",
    "        elif object_name == \"Driveway.obj\":\n",
    "            new_X = pointCloud[:,0] - 25\n",
    "            new_Z = pointCloud[:,2] - 50\n",
    "            new_vX = original_vertex[0] - 25\n",
    "            new_vZ = original_vertex[2] - 50\n",
    "        elif object_name == \"Apartment.obj\":\n",
    "            new_X = pointCloud[:,0] + 25\n",
    "            new_Z = pointCloud[:,2] - 50\n",
    "            new_vX = original_vertex[0] + 25\n",
    "            new_vZ = original_vertex[2] - 50\n",
    "        elif object_name == \"Workstations-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] - 50\n",
    "            new_Z = pointCloud[:,2] + 0\n",
    "            new_vX = original_vertex[0] - 50\n",
    "            new_vZ = original_vertex[2] + 0\n",
    "        elif object_name == \"Kitchen-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] + 0\n",
    "            new_Z = pointCloud[:,2] + 0\n",
    "            new_vX = original_vertex[0] + 0\n",
    "            new_vZ = original_vertex[2] + 0\n",
    "        elif object_name == \"HallWayToKitchen-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] - 25\n",
    "            new_Z = pointCloud[:,2] + 50\n",
    "            new_vX = original_vertex[0] - 25\n",
    "            new_vZ = original_vertex[2] + 50\n",
    "        elif object_name == \"StairWell-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] + 25\n",
    "            new_Z = pointCloud[:,2] + 50\n",
    "            new_vX = original_vertex[0] + 25\n",
    "            new_vZ = original_vertex[2] + 50\n",
    "        else:\n",
    "            print(\"Error:\",obj_meta)\n",
    "\n",
    "        new_Y = pointCloud[:,1]\n",
    "\n",
    "        new_partial_pointcloud = np.stack((new_X,new_Z,new_Y)).T\n",
    "        \n",
    "        max_known_span = max(np.amax(new_partial_pointcloud, axis = 0) - np.amin(new_partial_pointcloud, axis = 0))\n",
    "        \n",
    "        nbrs = NearestNeighbors(n_neighbors=min(2*num_points,len(new_partial_pointcloud)), algorithm='kd_tree').fit(new_partial_pointcloud)\n",
    "\n",
    "        # Get submap \"centroids\" by quantizing by 0.25m, i.e. round then unique\n",
    "        if max_known_span > 3*spatial_span:\n",
    "            round_new_partial_pointcloud = 100*np.around(0.01*new_partial_pointcloud,decimals=2)\n",
    "            unq_round_partial_pointcloud = np.unique(round_new_partial_pointcloud[:,:3],axis = 0)\n",
    "        #    \n",
    "            raw_partial_centroids = unq_round_partial_pointcloud\n",
    "            c_nbrs = NearestNeighbors(n_neighbors = min(25,len(raw_partial_centroids)),  algorithm='kd_tree').fit(raw_partial_centroids)\n",
    "            c_dist, c_ind = c_nbrs.kneighbors(raw_partial_centroids)\n",
    "\n",
    "            ia1, ia2 = np.where(c_dist < 1.73)\n",
    "            \n",
    "\n",
    "            dist_bins = np.bincount(ia1)\n",
    "            max_dist = max(np.bincount(ia1))\n",
    "            raw_partial_centroids = raw_partial_centroids[[i for i, j in enumerate(dist_bins) if j == max_dist]]\n",
    "            \n",
    "            raw_partial_centroids = raw_partial_centroids+np.random.normal(0,interval,raw_partial_centroids.shape)\n",
    "            \n",
    "        else:\n",
    "            # Correcting this, because the attacker is supposed to not know the true centroid\n",
    "            # and has to estimate it instead.\n",
    "            #raw_partial_centroids = [[new_vX, new_vZ, original_vertex[1]]]        \n",
    "\n",
    "            raw_partial_centroids = [np.mean(new_partial_pointcloud, axis = 0)]\n",
    "            \n",
    "        for northing, easting, alting in raw_partial_centroids:\n",
    "            \n",
    "            # Getting the points around our centroid defined by [northing, easting]\n",
    "            distances, indices = nbrs.kneighbors([[northing, easting, alting]])\n",
    "            \n",
    "            #if max(distances[0]) < 0.5*spatial_span: continue\n",
    "                \n",
    "            submap_pointcloud = new_partial_pointcloud[indices[0,np.where(distances[0,:]<=spatial_span)[0]]]\n",
    "\n",
    "            if len(submap_pointcloud) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Centering and rescaling\n",
    "            submap_pointcloud = (submap_pointcloud - [northing, easting, alting])/spatial_span\n",
    "\n",
    "            if len(submap_pointcloud) > num_points:\n",
    "                submap_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points)]\n",
    "            elif len(submap_pointcloud) < num_points and len(submap_pointcloud) >= cutoff*num_points :\n",
    "                #print(i,submap_pointcloud.shape)\n",
    "                additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud))]\n",
    "                additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "                submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "            elif len(submap_pointcloud) < cutoff*num_points :\n",
    "                #continue\n",
    "                #print(i,submap_pointcloud.shape)\n",
    "                additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud), True)]\n",
    "                additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "                submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "\n",
    "            timestamp = int(10**16*(time.time()))\n",
    "            \n",
    "\n",
    "            csv_writer.writerow([timestamp,northing,easting,alting,obj_])\n",
    "            \n",
    "            with open(pointcloud_partial_path+'/{}.pickle'.format(timestamp),'wb') as f:\n",
    "                pickle.dump(submap_pointcloud.T,f)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    print(\"   Done with submap generation for radius {} ( {} samples) in {:.3f} seconds\".format(radius,count,time.time()-t1))\n",
    "    csvfile.close()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Successive release of RANSAC partial spaces\n",
    "\n",
    "baseline_path = 'pointnetvlad_submaps/'\n",
    "\n",
    "skip = 5\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "with open('successive_sample_points_per_release.pickle','rb') as f:\n",
    "    successive_sample_points_per_release = pickle.load(f)\n",
    "\n",
    "for radius in np.arange(0.5,2.1,0.5):\n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    successive_path = os.path.join(baseline_path,\"successive_radius_\"+str(radius))    \n",
    "    \n",
    "    if not os.path.exists(successive_path): os.mkdir(successive_path)\n",
    "    \n",
    "    for i in np.arange(1,100,skip): # releases\n",
    "        \n",
    "        successive_release_path = os.path.join(successive_path,\"release_\"+str(i))\n",
    "        pointcloud_successive_path = os.path.join(successive_release_path,\"pointcloud_4m\")\n",
    "        \n",
    "        if not os.path.exists(successive_release_path): os.mkdir(successive_release_path)\n",
    "        if not os.path.exists(pointcloud_successive_path): os.mkdir(pointcloud_successive_path)\n",
    "                                    \n",
    "        csvfile = open(successive_release_path+\"/pointcloud_centroids_4m.csv\",'w',newline = '')\n",
    "\n",
    "        csv_writer = csv.writer(csvfile, delimiter = ',')\n",
    "        csv_writer.writerow(['timestamp', 'northing', 'easting', 'alting','obj'])\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        for successive_sample_points_per_release_per_obj in successive_sample_points_per_release[i]:\n",
    "    \n",
    "            #print(\"  \",len(successive_sample_points_per_release_per_obj),\"releases\")\n",
    "            growing_point_cloud = []\n",
    "            \n",
    "            new_vX = []\n",
    "            new_vZ = []\n",
    "            \n",
    "            ransac_pointCloud = []\n",
    "                \n",
    "            for [obj_, object_name, original_vertex], reference_ransac in successive_sample_points_per_release_per_obj[:i]:\n",
    "\n",
    "                try:\n",
    "                    if len(ransac_pointCloud) == 0: # if empty, open. This only happens at beginning\n",
    "                        with open(\"../ransac_pc/ransac_point_collection_{}.pickle\".format(reference_ransac),'rb') as f:\n",
    "                            ransac_trial_point_collection = pickle.load(f)\n",
    "\n",
    "                        object_, ransac_pointCloud, tri_ = ransac_trial_point_collection[int(obj_)]\n",
    "                        \n",
    "                        ransac_nbrs = NearestNeighbors(n_neighbors=min(20000,len(ransac_pointCloud)), algorithm='kd_tree').fit(ransac_pointCloud[:,:3])\n",
    "\n",
    "                except:\n",
    "                    print(\"Can't get ransac samples for\",i,obj_, object_name)\n",
    "                    continue\n",
    "\n",
    "                dist_, ind_ = ransac_nbrs.kneighbors([original_vertex[:3]])\n",
    "                pointCloud =  ransac_pointCloud[ind_[0,np.where(dist_[0,:]<=radius)[0]]]\n",
    "\n",
    "                if len(pointCloud) == 0: continue\n",
    "                    \n",
    "                #Regular Accumulation\n",
    "                if len(growing_point_cloud) == 0:\n",
    "                    growing_point_cloud = pointCloud\n",
    "\n",
    "                else:\n",
    "                    growing_point_cloud = np.concatenate(\n",
    "                        (growing_point_cloud,pointCloud),\n",
    "                        axis=0\n",
    "                    )\n",
    "                    \n",
    "            if len(growing_point_cloud) == 0: continue\n",
    "                    \n",
    "            pointCloud = np.unique(growing_point_cloud,axis=0)     \n",
    "\n",
    "            if object_name == \"Reception-Data61-L5.obj\":\n",
    "                new_X = pointCloud[:,0] + 50\n",
    "                new_Z = pointCloud[:,2] + 0\n",
    "                new_vX = original_vertex[0] + 50\n",
    "                new_vZ = original_vertex[2] + 0\n",
    "            elif object_name == \"Driveway.obj\":\n",
    "                new_X = pointCloud[:,0] - 25\n",
    "                new_Z = pointCloud[:,2] - 50\n",
    "                new_vX = original_vertex[0] - 25\n",
    "                new_vZ = original_vertex[2] - 50\n",
    "            elif object_name == \"Apartment.obj\":\n",
    "                new_X = pointCloud[:,0] + 25\n",
    "                new_Z = pointCloud[:,2] - 50\n",
    "                new_vX = original_vertex[0] + 25\n",
    "                new_vZ = original_vertex[2] - 50\n",
    "            elif object_name == \"Workstations-Data61-L4.obj\":\n",
    "                new_X = pointCloud[:,0] - 50\n",
    "                new_Z = pointCloud[:,2] + 0\n",
    "                new_vX = original_vertex[0] - 50\n",
    "                new_vZ = original_vertex[2] + 0\n",
    "            elif object_name == \"Kitchen-Data61-L4.obj\":\n",
    "                new_X = pointCloud[:,0] + 0\n",
    "                new_Z = pointCloud[:,2] + 0\n",
    "                new_vX = original_vertex[0] + 0\n",
    "                new_vZ = original_vertex[2] + 0\n",
    "            elif object_name == \"HallWayToKitchen-Data61-L4.obj\":\n",
    "                new_X = pointCloud[:,0] - 25\n",
    "                new_Z = pointCloud[:,2] + 50\n",
    "                new_vX = original_vertex[0] - 25\n",
    "                new_vZ = original_vertex[2] + 50\n",
    "            elif object_name == \"StairWell-Data61-L4.obj\":\n",
    "                new_X = pointCloud[:,0] + 25\n",
    "                new_Z = pointCloud[:,2] + 50\n",
    "                new_vX = original_vertex[0] + 25\n",
    "                new_vZ = original_vertex[2] + 50\n",
    "            else:\n",
    "                print(\"Error:\",[obj_, object_name, original_vertex])\n",
    "\n",
    "            new_Y = pointCloud[:,1]\n",
    "\n",
    "            new_partial_pointcloud = np.stack((new_X,new_Z,new_Y)).T\n",
    "\n",
    "            nbrs = NearestNeighbors(n_neighbors=min(2*num_points,len(new_partial_pointcloud)), algorithm='kd_tree').fit(new_partial_pointcloud)\n",
    "\n",
    "            # Get submap \"centroids\" by quantizing by 0.25m, i.e. round then unique\n",
    "            #if radius > spatial_span:\n",
    "            round_new_partial_pointcloud = 100*np.around(0.01*new_partial_pointcloud,decimals=2)\n",
    "            unq_round_partial_pointcloud = np.unique(round_new_partial_pointcloud[:,:3],axis = 0)\n",
    "        #    \n",
    "            partial_centroids = unq_round_partial_pointcloud+np.random.normal(0,interval,unq_round_partial_pointcloud.shape)\n",
    "            c_nbrs = NearestNeighbors(n_neighbors = min(25,len(partial_centroids)),  algorithm='kd_tree').fit(partial_centroids)\n",
    "            c_dist, c_ind = c_nbrs.kneighbors(partial_centroids)\n",
    "\n",
    "            ia1, ia2 = np.where(c_dist < 1.73)\n",
    "\n",
    "            dist_bins = np.bincount(ia1)\n",
    "            max_dist = max(np.bincount(ia1))\n",
    "            partial_centroids = partial_centroids[[i for i, j in enumerate(dist_bins) if j == max_dist]]\n",
    "\n",
    "            for northing, easting, alting in partial_centroids:\n",
    "\n",
    "                # Getting the points around our centroid defined by [northing, easting]\n",
    "                distances, indices = nbrs.kneighbors([[northing, easting, alting]])\n",
    "\n",
    "                #if max(distances[0]) < 0.5*spatial_span: continue\n",
    "\n",
    "                submap_pointcloud = new_partial_pointcloud[indices[0,np.where(distances[0,:]<=spatial_span)[0]]]\n",
    "\n",
    "                if len(submap_pointcloud) == 0:\n",
    "                    continue\n",
    "\n",
    "                # Centering and rescaling\n",
    "                submap_pointcloud = (submap_pointcloud - [northing, easting, alting])/spatial_span\n",
    "                #per_radius_partial_length.append([northing, easting, alting, len(submap_pointcloud)])\n",
    "\n",
    "                if len(submap_pointcloud) > num_points:\n",
    "                    submap_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points)]\n",
    "                elif len(submap_pointcloud) < num_points and len(submap_pointcloud) >= cutoff*num_points :\n",
    "                    #print(i,submap_pointcloud.shape)\n",
    "                    additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud))]\n",
    "                    additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "                    submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "                elif len(submap_pointcloud) < cutoff*num_points :\n",
    "                    #continue\n",
    "                    #print(i,submap_pointcloud.shape)\n",
    "                    additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud), True)]\n",
    "                    additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "                    submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "\n",
    "                timestamp = int(10**16*(time.time()))\n",
    "\n",
    "                csv_writer.writerow([timestamp,northing,easting,alting,obj_])\n",
    "\n",
    "                with open(pointcloud_successive_path+'/{}.pickle'.format(timestamp),'wb') as f:\n",
    "                    pickle.dump(submap_pointcloud.T,f)\n",
    "                    \n",
    "                count += 1\n",
    "                    \n",
    "        if i % 10 == 1:\n",
    "            print(\"   Done with submap generation for iteration {}, radius {} ({} submaps) in {:.3f} seconds\".format(i,radius,count,time.time()-t1))\n",
    "            t1 = time.time()\n",
    "\n",
    "        csvfile.close()\n",
    "        \n",
    "    print(\" Done with generalized submap generation for radius {} in {:.3f} seconds\".format(radius,time.time()-t0))\n",
    "    t0 = time.time()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0.3.4: Building database and query files for evaluation with pointnetVLAD \n",
    " - the combined Raw and RANSAC referece database \n",
    " - for validation with one-time released Raw spaces\n",
    " - for testing with one-time released RANSAC spaces\n",
    " - for testing with successive RANSAC spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path= \"pointnetvlad_submaps/\"#\"../partial_dataset/\"\n",
    "\n",
    "construct_query_and_database_sets(\n",
    "    base_path, \n",
    "    ['raw_dataset', 'ransac_dataset'], \n",
    "    \"/pointcloud_4m_0.25/\",\n",
    "    \"pointcloud_centroids_4m_0.25.csv\")#, all_folders[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For validation with raw queries.\n",
    "\n",
    "for radius in np.arange(0.25,3.1,0.25):\n",
    "    \n",
    "    partial_path = 'raw_partial_radius_'+str(radius)+\"_4096_unassisted\"#\n",
    "    \n",
    "    print(partial_path)\n",
    "    construct_query_sets(partial_path, \"/pointcloud_4m/\", \"pointcloud_centroids_4m.csv\")#, all_folders[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For the Ransac queries.\n",
    "\n",
    "for radius in np.arange(0.25,3.1,0.25):\n",
    "    \n",
    "    partial_path = 'ransac_partial_radius_'+str(radius)+\"_4096_unassisted\"#\n",
    "    \n",
    "    print(partial_path)\n",
    "    construct_query_sets(partial_path, \"/pointcloud_4m/\", \"pointcloud_centroids_4m.csv\")#, all_folders[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For the successive queries.\n",
    "\n",
    "successive_dir = os.path.join(base_path,\"successive_queries\")    \n",
    "\n",
    "if not os.path.exists(successive_dir): os.mkdir(successive_dir)\n",
    "\n",
    "for radius in np.arange(0.5,2.1,0.5):\n",
    "    \n",
    "    successive_path = 'successive_radius_'+str(radius)\n",
    "    \n",
    "    for release in np.arange(1,100,5):\n",
    "    \n",
    "        partial_path = 'release_'+str(release)\n",
    "\n",
    "        print(partial_path)\n",
    "        construct_successive_query_sets(successive_path,partial_path, \"/pointcloud_4m/\", \"pointcloud_centroids_4m.csv\")#, all_folders[index])\n",
    "\n",
    "#print(all_folders)\n",
    "#print(\"training:\",train_folders)\n",
    "#\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-3d_env] *",
   "language": "python",
   "name": "conda-env-.conda-3d_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
