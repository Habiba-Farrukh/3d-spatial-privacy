{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import time\n",
    "import h5py\n",
    "import csv\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "\n",
    "from numpy import linalg as LA\n",
    "from scipy.spatial import Delaunay\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#sys.path.insert(0, \"../\")\n",
    "from info3d import *\n",
    "from nn_matchers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global parameters\n",
    "\n",
    "radius_range = np.arange(0.5,1.6,0.5)\n",
    "\n",
    "with open('point_collection/new_contiguous_point_collection.pickle','rb') as f: \n",
    "    new_contiguous_point_collection = pickle.load(f)\n",
    "\n",
    "point_collection_indices = np.arange(len(new_contiguous_point_collection))\n",
    "point_collection_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Getting sample points for one-time partial radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating 1000 samples in 0.123 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([153., 138., 141., 137., 136., 150., 145.]),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       " <a list of 7 Patch objects>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAeWUlEQVR4nO3deZxcdZ3u8c8DYV+MkhYhC0FEEXmpcFvAHQWVTeJ1VOC6RMSJqIO4IrjhxhXGjevMVcwAAoosggruIrKpA5ggyI4BQgiJpCNbWASiz/xxfn0omupOpburqjv9vF+venWdpc75dnV1PfX7/c45JdtEREQArNXtAiIiYuxIKERERC2hEBERtYRCRETUEgoREVFLKERERC2hMIFIOl7Sp0dpWzMkPSBp7TJ9kaR3j8a2y/Z+IWn2aG1vNfb7RUnLJf21xfU/K+l77a6rUyS9U9Lvul1HdE9CYQ0haaGkhyWtkHSvpD9IOkRS/Te2fYjtL7S4rT2GWsf2Itsb2/7HKNT+pDdW23vZPmWk217NOqYDHwG2t/2MJst3k7S4kzVFdFpCYc3yetubAFsBxwAfB04c7Z1ImjTa2xwjtgL+ZntZtwuJ6JaEwhrI9n22zwP2B2ZL2gFA0smSvljuT5H009KquFvSpZLWkvRdYAbwk9I9dLikmZIs6WBJi4DfNsxrDIhtJF0h6T5J50p6WtnXkz5h97dGJO0JfALYv+zv6rK87o4qdX1K0u2Slkk6VdJTyrL+OmZLWlS6fj452HMj6Snl8X1le58q298DOB/YstRx8oDHbQT8omH5A5K2LIvXLdtcIek6Sb0Nj9tS0jllf7dJ+sAQte0t6fqynTslfbTMf2r5W/VJuqfcn9bwuItKt9cfSl0/kbSZpNMk3S/pj5JmNqxvSR+QdGt5vr7c2KIcUNN2ks4vr5GbJL1liPrfWba5ovyub22Y/3tJ/1FeGzdK2r3hcQdJuqE87lZJ7xmw3VmSriq/yy3lNdP/tzxR0tLyfH1Rj3dnPkvSxWV/yyWdOVjdMYDt3NaAG7AQ2KPJ/EXAe8v9k4EvlvtfAo4H1im3lwNqti1gJmDgVGAjYIOGeZPKOhcBdwI7lHXOAb5Xlu0GLB6sXuCz/es2LL8IeHe5/y5gAfBMYGPgh8B3B9T2X6WuFwCPAM8d5Hk6FTgX2KQ89mbg4MHqHPDYZr/HZ4G/A3sDa5fn9bKybC1gPvAZYN1S/63A6wbZ/lLg5eX+U4Gdyv3NgH8BNix1/wD48YDnagGwDfAU4Prye+0BTCq/83ca1jdwIfA0qg8ANzc81+8EflfubwTcARxUtrMTsBx4XpPaNwLuB55TprfoX69scyXwIarX2v7AfcDTyvJ9Su0CXgk81PC771zWfU15PqcC25VlPwa+Xfb9dOAK4D1l2enAJ8tj1gde1u3/0fFyS0thzbeE6p9/oMeo/nG3sv2Y7Utd/puG8FnbD9p+eJDl37V9re0HgU8Db+n/5DZCbwW+ZvtW2w8ARwIHDGilfM72w7avBq6mCocnKLXsDxxpe4XthcBXgbePsL7f2f65q/GV7zbs+0VAj+3P237U9q1U4XXAINt5DNhe0qa277F9JYDtv9k+x/ZDtlcAR1O9eTb6ju1bbN9H1aK5xfZvbK+kCpEdB6x/rO27bS8CjgMObFLPvsBC29+xvbLUcw7wpkHq/yewg6QNbC+1fV3DsmXAceW1diZwE1UYYPtnpXbbvhj4NdWHFICDgZNsn2/7n7bvtH2jpM2BvYAPltfkMuDrDc/tY1TdgVva/rvtDJ63KKGw5psK3N1k/pepPl3+ujTZj2hhW3esxvLbqT4VTmmpyqFtWbbXuO1JwOYN8xqPFnqIqkUx0BSqT+wDtzV1hPUN3Pf6JbC2oupuurf/RtVVtnmzjVC1BvYGbi9dHy8GkLShpG+X7q77gUuAyQMC966G+w83mR74fAz8W23Jk20F7DKg/rcCTxqELx8E9gcOAZZK+pmk7RpWuXPAh456n5L2knRZ6aK6tzwH/a+b6cAtg9S2TtlXf23fpmoxABxO1fK4onTpvavJNqKJhMIaTNKLqN7wnvQpqXxS/ojtZwKvBz7c0M87WIthVS2J6Q33Z1B9WlsOPEjV9dFf19pAz2psdwnVm0DjtlfyxDe+Vizn8U+Qjdu6s8XHr+4lhe8AbrM9ueG2ie29m27c/qPtWVRvbD8GziqLPgI8B9jF9qbAK8p8rWY9jQb+rZYMUv/FA+rf2PZ7B6n/V7ZfQ9UCvZGqVdRvqqTGemcASyStR9X6+Aqwue3JwM95/He7g6prqVltjwBTGmrb1PbzSi1/tf2vtrcE3gN8U9KzhnpCopJQWANJ2lTSvsAZVH311zRZZ98yGCeqvuB/lBtUb7bPHMau3yZpe0kbAp8Hzi5dKjdTfXreR9I6wKeA9Roedxcwc7DBTqr+4Q9J2lrSxsD/Bc4sXSMtK7WcBRwtaRNJWwEfBlo9z+AuYDOVQe4WXAHcL+njkjaQtLakHUpYP4GkdSW9VdJTbD/G438TqMYRHgbuVTV4f1SL+x/Kx1QNYE8HDgOaDcT+FHi2pLdLWqfcXiTpuU3q31zSfqoG5B8BHmioH6qg+0DZxpuB51K9+a9L9VroA1ZK2gt4bcPjTgQOkrS7qgMCpkrazvZSqm6mr5bX+1qStpH0ylLPm/X4YPw9VIE+4sOnJ4KEwprlJ5JWUH2K+iTwNapBwma2BX5D9c/738A3bV9Uln0J+FRpln90Nfb/XarB7L9SDe59AKqjoYD3ASdQfSp/EGg8GukH5effJF3ZZLsnlW1fAtxGNbB76GrU1ejQsv9bqVpQ3y/bXyXbN1IF1K3luWnW5dK4/j+oWmEvLHUvp3oOBguVtwMLSxfRIcDbyvzjqAbRlwOXAb9spd5VOJdqEPwq4Gc0OXS5jF+8lqqffgnV3/VYnhjo/daiatEsoequfCXV37zf5VSvueVUYyJvKmMlK6heJ2dRvXn/H+C8hhquoHoNf51qwPliHm/pvYMqVK4vjz2bqpUC1XjO5ZIeKNs7zPZtLT0zE1z/0SYRMUFIMrCt7QUd2t87qY5uelkn9hcjk5ZCRETUEgoREVFrWyhIOknV2afXDph/qKozI6+T9O8N84+UtKAse1276oqY6GyrU11HZX8np+to/GjnNWxOBv6T6mxKACS9CpgFPN/2I5KeXuZvTzWY9TyqY5d/I+nZHoWLrUVEROvaFgq2L1HD9VaK9wLH2H6krNN/4bFZwBll/m2SFlCd3v7fQ+1jypQpnjlz4C4iImIo8+fPX267p9myTl/t8tnAyyUdTXVY4Udt/5HqBKvLGtZbzCBnmUqaA8wBmDFjBvPmzWtvxRERaxhJtw+2rNMDzZOoLvS1K/Ax4Kxy8lSzMzObHitre67tXtu9PT1Ngy4iIoap06GwGPhhufDVFVQX0JpS5jeedj+N5qfdR0REG3U6FH4MvBpA0rOpzkZcTnXG4QGS1pO0NdWZj1d0uLaIiAmvbWMKkk6nuv78FFVfsHIU1eUETiqHqT4KzC5XTrxO0llUp6uvBN6fI48iIjpvXF/more31xlojohYPZLm2+5ttixnNEdERC2hEBERtYRCRETUEgoREVHr9BnNY8bMI37W7RJWy8Jj9ul2CRExAaSlEBERtYRCRETUEgoREVGbsGMKEdF+42nsLuN2lbQUIiKillCIiIhaQiEiImoJhYiIqCUUIiKillCIiIhaDkmNiGB8HT4L7TuENi2FiIioJRQiIqLWtlCQdJKkZeX7mAcu+6gkS5pSpiXpG5IWSPqzpJ3aVVdERAyunS2Fk4E9B86UNB14DbCoYfZewLblNgf4VhvrioiIQbRtoNn2JZJmNln0deBw4NyGebOAU20buEzSZElb2F7arvqifTJgFzF+dXRMQdJ+wJ22rx6waCpwR8P04jKv2TbmSJonaV5fX1+bKo2ImJg6FgqSNgQ+CXym2eIm89xsO7bn2u613dvT0zOaJUZETHidPE9hG2Br4GpJANOAKyXtTNUymN6w7jRgSQdri4gIOhgKtq8Bnt4/LWkh0Gt7uaTzgH+TdAawC3BfxhOeaLz100fE+NS2UJB0OrAbMEXSYuAo2ycOsvrPgb2BBcBDwEHtqitioPEUuBkUj3Zr59FHB65i+cyG+wbe365aIiKiNbn2UcQ4Mp5aNTE+5TIXERFRSyhEREQtoRAREbWEQkRE1BIKERFRSyhEREQtoRAREbWEQkRE1BIKERFRSyhEREQtoRAREbWEQkRE1BIKERFRSyhEREQtoRAREbWEQkRE1BIKERFRa1soSDpJ0jJJ1zbM+7KkGyX9WdKPJE1uWHakpAWSbpL0unbVFRERg2tnS+FkYM8B884HdrD9fOBm4EgASdsDBwDPK4/5pqS121hbREQ00bZQsH0JcPeAeb+2vbJMXgZMK/dnAWfYfsT2bcACYOd21RYREc11c0zhXcAvyv2pwB0NyxaXeU8iaY6keZLm9fX1tbnEiIiJpSuhIOmTwErgtP5ZTVZzs8fanmu713ZvT09Pu0qMiJiQJnV6h5JmA/sCu9vuf+NfDExvWG0asKTTtUVETHQdbSlI2hP4OLCf7YcaFp0HHCBpPUlbA9sCV3SytoiIaGNLQdLpwG7AFEmLgaOojjZaDzhfEsBltg+xfZ2ks4DrqbqV3m/7H+2qLSIimmtbKNg+sMnsE4dY/2jg6HbVExERq5YzmiMiopZQiIiIWkIhIiJqCYWIiKglFCIiopZQiIiIWkIhIiJqCYWIiKglFCIiopZQiIiIWkIhIiJqCYWIiKglFCIiopZQiIiIWkIhIiJqLYeCpI3aWUhERHTfKkNB0kskXQ/cUKZfIOmbba8sIiI6rpWWwteB1wF/A7B9NfCKdhYVERHd0VL3ke07Bsxa5fcnSzpJ0jJJ1zbMe5qk8yX9pfx8apkvSd+QtEDSnyXttFq/RUREjIpWQuEOSS8BLGldSR+ldCWtwsnAngPmHQFcYHtb4IIyDbAXsG25zQG+1cL2IyJilLUSCocA7wemAouBF5bpIdm+BLh7wOxZwCnl/inAGxrmn+rKZcBkSVu0UFtERIyiSatawfZy4K2jtL/NbS8t210q6ell/lSgsYtqcZm3dOAGJM2hak0wY8aMUSorIiKgtaOPTpE0uWH6qZJOGuU61GSem61oe67tXtu9PT09o1xGRMTE1kr30fNt39s/YfseYMdh7u+u/m6h8nNZmb8YmN6w3jRgyTD3ERERw9RKKKzVf5QQVEcQ0UK30yDOA2aX+7OBcxvmv6MchbQrcF9/N1NERHROK2/uXwX+IOnsMv1m4OhVPUjS6cBuwBRJi4GjgGOAsyQdDCwq2wL4ObA3sAB4CDhoNX6HiIgYJa0MNJ8qaT7wKqq+/zfavr6Fxx04yKLdm6xrWjiiKSIi2qulbiDb10nqA9YHkDTD9qK2VhYRER3XytFH+0n6C3AbcDGwEPhFm+uKiIguaGWg+QvArsDNtrem6v75fVurioiIrmglFB6z/Teqo5DWsn0h1VnNERGxhmllTOFeSRsDlwKnSVoGrGxvWRER0Q2ttBRmAQ8DHwR+CdwCvL6dRUVERHe0ckjqg5KeAexMdYG7X5XupIiIWMO0cvTRu4ErgDcCbwIuk/SudhcWERGd18qYwseAHftbB5I2A/4AjPZF8SIiostaGVNYDKxomF7BEy9zHRERa4hWWgp3ApdLOpfqctazgCskfRjA9tfaWF9ERHRQK6FwS7n167+y6SajX05ERHRTK0cffa7/vqS1gI1t39/WqiIioitaOfro+5I2lbQRcD1wk6SPtb+0iIjotFYGmrcvLYM3UH3vwQzg7W2tKiIiuqKVUFhH0jpUoXCu7ccY5PuTIyJifGslFL5NdbnsjYBLJG0FZEwhImINtMpQsP0N21Nt712+IW0R1bewRUTEGqaVlsITuDKiq6RK+pCk6yRdK+l0SetL2lrS5ZL+IulMSeuOZB8REbH6VjsURkrSVOADQK/tHYC1gQOAY4Gv294WuAc4uNO1RURMdB0PhWISsIGkScCGwFLg1cDZZfkpVAPbERHRQa2cp7ChpE9L+q8yva2kfYe7Q9t3Al+hGptYCtwHzAfubeiWWgxMHe4+IiJieFppKXwHeAR4cZleDHxxuDuU9FSq6ydtDWxJdVTTXk1WbXrYq6Q5kuZJmtfX1zfcMiIioolWQmEb2/8OPAZg+2FAI9jnHsBttvvKOQ8/BF4CTC7dSQDTgCXNHmx7ru1e2709PT0jKCMiIgZqJRQelbQB5ZO7pG2oWg7DtQjYtXRLCdid6vIZF1J9iQ/AbB6/8F5ERHRIK6FwFNV3M0+XdBpwAXD4cHdo+3KqAeUrgWtKDXOBjwMflrQA2Aw4cbj7iIiI4WnlKqnnS7oS2JWq2+gw28tHslPbR1GFTaNbqb4HOiIiumTQUJC004BZS8vPGZJm2L6yfWVFREQ3DNVS+OoQy0x1XkFERKxBBg0F27m+UUTEBLPKMQVJ6wPvA15G1UK4FDje9t/bXFtERHRYK9/RfCqwAviPMn0g8F3gze0qKiIiuqOVUHiO7Rc0TF8o6ep2FRQREd3TynkKf5K0a/+EpF2A37evpIiI6JZWWgq7AO+QtKhMzwBukHQN1dcrPL9t1UVEREe1Egp7tr2KiIgYE1o5o/n2cmXT6Y3r5+S1iIg1TyuHpH4BeCdwC49fzjonr0VErIFa6T56C9Xlsx9tdzEREdFdrRx9dC0wud2FRERE97XSUvgS1WGp19LwPQq292tbVRER0RWthMIpwLFU333wz/aWExER3dRKKCy3/Y22VxIREV3XSijMl/Ql4Dye2H2UQ1IjItYwrYTCjuXnrg3zckhqRMQaqJWT10b9exUkTQZOAHagCph3ATcBZwIzgYXAW2zfM9r7joiIwbXSUkDSPsDzgPX759n+/Aj2+/+AX9p+k6R1gQ2BTwAX2D5G0hHAEcDHR7CPiIhYTas8T0HS8cD+wKGAqL5HYavh7lDSpsArgBMBbD9q+15gFtWRTpSfbxjuPiIiYnhaOXntJbbfAdxj+3PAi6mugzRczwT6gO9I+pOkEyRtBGxueylA+fn0EewjIiKGoZVQeLj8fEjSlsBjwNYj2OckYCfgW7Z3BB6k6ipqiaQ5kuZJmtfX1zeCMiIiYqBWQuGnZWD4y8CVVIPAp49gn4uBxbYvL9NnU4XEXZK2ACg/lzV7sO25tntt9/b09IygjIiIGGiVoWD7C7bvtX0O1VjCdrY/M9wd2v4rcIek55RZuwPXU50HMbvMmw2cO9x9RETE8Ax69JGkFwF3lDdxJL0D+BfgdkmftX33CPZ7KHBaOfLoVuAgqoA6S9LBwCKqAe2IiOigoQ5J/TawB4CkVwDHUL2ZvxCYC7xpuDu1fRXQ22TR7sPdZkREjNxQobB2Q2tgf2Bu6UI6R9JV7S8tIiI6bagxhbUl9YfG7sBvG5a1dNJbRESML0O9uZ8OXCxpOdVhqZcCSHoWcF8HaouIiA4bNBRsHy3pAmAL4Ne2+7+feS2qsYWIiFjDDNkNZPuyJvNubl85ERHRTa2cvBYRERNEQiEiImoJhYiIqCUUIiKillCIiIhaQiEiImoJhYiIqCUUIiKillCIiIhaQiEiImoJhYiIqCUUIiKillCIiIha10JB0tqS/iTpp2V6a0mXS/qLpDPL9zdHREQHdbOlcBhwQ8P0scDXbW8L3AMc3JWqIiImsK6EgqRpwD7ACWVawKuBs8sqpwBv6EZtERETWbdaCscBhwP/LNObAffaXlmmFwNTu1FYRMRE1vFQkLQvsMz2/MbZTVZ1k3lImiNpnqR5fX19bakxImKi6kZL4aXAfpIWAmdQdRsdB0yW1P/1oNOAJc0ebHuu7V7bvT09PZ2oNyJiwuh4KNg+0vY02zOBA4Df2n4rcCHwprLabODcTtcWETHRjaXzFD4OfFjSAqoxhhO7XE9ExIQzadWrtI/ti4CLyv1bgZ27WU9ExEQ3lloKERHRZQmFiIioJRQiIqKWUIiIiFpCISIiagmFiIioJRQiIqKWUIiIiFpCISIiagmFiIioJRQiIqKWUIiIiFpCISIiagmFiIioJRQiIqKWUIiIiFpCISIiagmFiIioJRQiIqLW8VCQNF3ShZJukHSdpMPK/KdJOl/SX8rPp3a6toiIia4bLYWVwEdsPxfYFXi/pO2BI4ALbG8LXFCmIyKigzoeCraX2r6y3F8B3ABMBWYBp5TVTgHe0OnaIiImuq6OKUiaCewIXA5sbnspVMEBPH2Qx8yRNE/SvL6+vk6VGhExIXQtFCRtDJwDfND2/a0+zvZc2722e3t6etpXYETEBNSVUJC0DlUgnGb7h2X2XZK2KMu3AJZ1o7aIiImsG0cfCTgRuMH21xoWnQfMLvdnA+d2uraIiIluUhf2+VLg7cA1kq4q8z4BHAOcJelgYBHw5i7UFhExoXU8FGz/DtAgi3fvZC0REfFEOaM5IiJqCYWIiKglFCIiopZQiIiIWkIhIiJqCYWIiKglFCIiopZQiIiIWkIhIiJqCYWIiKglFCIiopZQiIiIWkIhIiJqCYWIiKglFCIiopZQiIiIWkIhIiJqCYWIiKiNuVCQtKekmyQtkHREt+uJiJhIxlQoSFob+P/AXsD2wIGStu9uVRERE8eYCgVgZ2CB7VttPwqcAczqck0RERPGpG4XMMBU4I6G6cXALo0rSJoDzCmTD0i6aZj7mgIsH+Zju2E81TueaoXxVe94qhXGV73jqVZ07Ijq3WqwBWMtFNRknp8wYc8F5o54R9I8270j3U6njKd6x1OtML7qHU+1wviqdzzVCu2rd6x1Hy0GpjdMTwOWdKmWiIgJZ6yFwh+BbSVtLWld4ADgvC7XFBExYYyp7iPbKyX9G/ArYG3gJNvXtWl3I+6C6rDxVO94qhXGV73jqVYYX/WOp1qhTfXK9qrXioiICWGsdR9FREQXJRQiIqI2IUNhPF1KQ9JJkpZJurbbtayKpOmSLpR0g6TrJB3W7ZoGI2l9SVdIurrU+rlu19QKSWtL+pOkn3a7lqFIWijpGklXSZrX7XpWRdJkSWdLurG8fl/c7ZqakfSc8pz23+6X9MFR3cdEG1Mol9K4GXgN1SGwfwQOtH19VwsbhKRXAA8Ap9reodv1DEXSFsAWtq+UtAkwH3jDWHxuJQnYyPYDktYBfgccZvuyLpc2JEkfBnqBTW3v2+16BiNpIdBre1ycDCbpFOBS2yeUIx83tH1vt+saSnkvuxPYxfbto7XdidhSGFeX0rB9CXB3t+tohe2ltq8s91cAN1CdpT7muPJAmVyn3Mb0JyRJ04B9gBO6XcuaRNKmwCuAEwFsPzrWA6HYHbhlNAMBJmYoNLuUxph84xrPJM0EdgQu724lgytdMVcBy4DzbY/ZWovjgMOBf3a7kBYY+LWk+eXSNGPZM4E+4Dula+4ESRt1u6gWHACcPtobnYihsMpLacTISNoYOAf4oO37u13PYGz/w/YLqc6c31nSmO2ek7QvsMz2/G7X0qKX2t6J6orH7y/doGPVJGAn4Fu2dwQeBMb6WOO6wH7AD0Z72xMxFHIpjTYq/fPnAKfZ/mG362lF6Sq4CNizy6UM5aXAfqWv/gzg1ZK+192SBmd7Sfm5DPgRVbftWLUYWNzQUjybKiTGsr2AK23fNdobnoihkEtptEkZvD0RuMH217pdz1Ak9UiaXO5vAOwB3NjdqgZn+0jb02zPpHrN/tb227pcVlOSNioHGlC6YV4LjNmj52z/FbhD0nPKrN2BMXdwxAAH0oauIxhjl7nohA5fSmPEJJ0O7AZMkbQYOMr2id2talAvBd4OXFP66gE+YfvnXaxpMFsAp5QjONYCzrI9pg/zHEc2B35UfUZgEvB927/sbkmrdChwWvmgeCtwUJfrGZSkDamOnnxPW7Y/0Q5JjYiIwU3E7qOIiBhEQiEiImoJhYiIqCUUIiKillCIiIhaQiFiFSQ9Q9IZkm6RdL2kn0t69ihufzdJLxmt7UWMREIhYgjlhLwfARfZ3sb29sAnqI7FHy27AQmFGBMSChFDexXwmO3j+2fYvgr4naQvS7q2fG/A/lB/6q9PgpP0n5LeWe4vlPQ5SVeWx2xXLhx4CPChcn38l3fwd4t4kgl3RnPEatqB6nshBnoj8ELgBcAU4I+SLmlhe8tt7yTpfcBHbb9b0vHAA7a/MmpVRwxTWgoRw/My4PRypdW7gIuBF7XwuP6LBM4HZraptohhSyhEDO064H81md/sEuwAK3ni/9X6A5Y/Un7+g7TUYwxKKEQM7bfAepL+tX+GpBcB9wD7ly/q6aH65q4rgNuB7SWtJ+kpVFfcXJUVwCajX3rE6ssnlYgh2Lak/w0cJ+kI4O/AQuCDwMbA1VRf0nR4uQQzks4C/gz8BfhTC7v5CXC2pFnAobYvHfVfJKJFuUpqRETU0n0UERG1hEJERNQSChERUUsoRERELaEQERG1hEJERNQSChERUfsfvDQKxjPo83EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = 1000\n",
    "\n",
    "sample_points = []\n",
    "\n",
    "samples_indeces = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in np.arange(samples):\n",
    "\n",
    "    random_object = np.random.choice(point_collection_indices)\n",
    "\n",
    "    object_name = new_contiguous_point_collection[random_object][0]\n",
    "    pointCloud = new_contiguous_point_collection[random_object][1]\n",
    "    triangles = new_contiguous_point_collection[random_object][2]\n",
    "    \n",
    "    triangle_index = np.random.choice(np.arange(len(triangles)))\n",
    "    vertex_index = triangles[triangle_index,1]\n",
    "    original_vertex = pointCloud[vertex_index]\n",
    "\n",
    "    sample_points.append([\n",
    "        random_object, \n",
    "        object_name, \n",
    "        original_vertex\n",
    "    ])\n",
    "    \n",
    "    samples_indeces.append(random_object)\n",
    "    \n",
    "print(\"Done generating\",len(sample_points),\"samples in {:.3f} seconds.\".format(time.time()-t0))\n",
    "\n",
    "with open('sample_points.pickle','wb') as f:\n",
    "    pickle.dump(sample_points,f)\n",
    "    \n",
    "plt.title(\"Distribution of the sample spaces\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Sample space\")\n",
    "plt.hist(samples_indeces,bins = np.arange(0,8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 Creating a synthetic set of successive partial spaces\n",
    "\n",
    "Similar to the partial case above, we use the same sample points, i.e. centroids, for successive releases but will only vary the size of the partial space for every release.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done with successive 1 sample_points extraction for radius 0.5 in 1.080 seconds\n",
      " Done with successive 34 sample_points extraction for radius 0.5 in 18.763 seconds\n",
      " Done with successive 67 sample_points extraction for radius 0.5 in 18.948 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "samples = 100\n",
    "releases = 100\n",
    "\n",
    "nearby_range = 2.0\n",
    "    \n",
    "t1 = time.time()\n",
    "\n",
    "successive_sample_points = []\n",
    "\n",
    "for i in np.arange(samples):#\n",
    "\n",
    "    random_object = np.random.choice(point_collection_indices)\n",
    "    #reference_ransac = np.random.randint(5)\n",
    "\n",
    "    object_name = new_contiguous_point_collection[random_object][0]\n",
    "    pointCloud = new_contiguous_point_collection[random_object][1]\n",
    "    triangles = new_contiguous_point_collection[random_object][2]\n",
    "\n",
    "    current_vertex = pointCloud[np.random.randint(len(pointCloud))]\n",
    "\n",
    "    growing_point_collection_vertices = [[\n",
    "        random_object, \n",
    "        object_name, \n",
    "        current_vertex\n",
    "    ]]\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=min(20000,len(pointCloud)),algorithm='kd_tree').fit(pointCloud[:,:3])\n",
    "\n",
    "    for release in np.arange(releases-1):\n",
    "\n",
    "        distances, indices = nbrs.kneighbors([current_vertex[:3]])\n",
    "\n",
    "        cand_indices = indices[0,np.where(distances[0]<(nearby_range))[0]]\n",
    "\n",
    "        distribution = np.sort(abs(np.random.normal(nearby_range*0.5,nearby_range*0.3,len(cand_indices))))\n",
    "\n",
    "        current_vertex = pointCloud[\n",
    "            np.random.choice(\n",
    "                cand_indices,\n",
    "                p = distribution/np.sum(distribution)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        growing_point_collection_vertices.append([\n",
    "            random_object, \n",
    "            object_name, \n",
    "            current_vertex\n",
    "        ])\n",
    "\n",
    "    successive_sample_points.append([\n",
    "        [random_object, object_name],\n",
    "        growing_point_collection_vertices\n",
    "    ])\n",
    "\n",
    "    if i % 33 == 1:\n",
    "        print(\" Done with successive {} sample_points extraction in {:.3f} seconds\".format(i,time.time()-t1))\n",
    "        t1 = time.time()\n",
    "\n",
    "    with open('successive_sample_points.pickle','wb') as f:\n",
    "        pickle.dump(successive_sample_points,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 samples for radius 0.5\n",
      "100 releases each\n",
      " Done with successive sample_points extraction in 2.226 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "t1 = time.time()\n",
    "\n",
    "try:\n",
    "    with open('successive_sample_points.pickle','rb') as f:\n",
    "        successive_point_collection = pickle.load(f)\n",
    "\n",
    "    samples = len(successive_point_collection)\n",
    "    releases = len(successive_point_collection[0][1])\n",
    "\n",
    "    print(samples,\"samples for radius\",radius)\n",
    "    print(releases,\"releases each\")\n",
    "\n",
    "except Exception as e1:\n",
    "    print(e1)\n",
    "\n",
    "successive_sample_points_per_release = [[]]\n",
    "\n",
    "for k, [obj_, growing_point_collection] in enumerate(successive_point_collection):\n",
    "\n",
    "    t2 = time.time()\n",
    "\n",
    "    successive_sample_points = []\n",
    "\n",
    "    reference_ransac = np.random.randint(5)\n",
    "\n",
    "    for i, obj_meta in enumerate(growing_point_collection):\n",
    "\n",
    "        successive_sample_points.append([obj_meta, reference_ransac])\n",
    "\n",
    "        try:\n",
    "            successive_sample_points_per_release[i].append(successive_sample_points)\n",
    "        except:\n",
    "            successive_sample_points_per_release.append([successive_sample_points])\n",
    "\n",
    "        #print(len(successive_sample_points_per_release[i]),len(successive_sample_points_per_release[i][k]))\n",
    "\n",
    "    with open('successive_sample_points_per_release.pickle','wb') as f:\n",
    "        pickle.dump(successive_sample_points_per_release,f)\n",
    "\n",
    "print(\" Done with successive sample_points extraction in {:.3f} seconds\".format(time.time()-t1))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Create submaps for pointnetvlad using same partial samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with submap generation for object (0) Reception-Data61-L5.obj in 13.064 seconds\n",
      "Done with submap generation for object (1) Driveway.obj in 29.068 seconds\n",
      "Done with submap generation for object (2) Apartment.obj in 49.137 seconds\n",
      "Done with submap generation for object (3) Workstations-Data61-L4.obj in 71.557 seconds\n",
      "Done with submap generation for object (4) Kitchen-Data61-L4.obj in 101.371 seconds\n",
      "Done with submap generation for object (5) HallWayToKitchen-Data61-L4.obj in 110.286 seconds\n",
      "Done with submap generation for object (6) StairWell-Data61-L4.obj in 133.973 seconds\n"
     ]
    }
   ],
   "source": [
    "# First, we need to create the reference dataset\n",
    "\n",
    "num_points = 4096\n",
    "\n",
    "baseline_path = 'pointnetvlad_submaps/'\n",
    "\n",
    "if not os.path.exists(baseline_path): os.mkdir(baseline_path)\n",
    "\n",
    "raw_path = os.path.join(baseline_path,\"raw_dataset\")\n",
    "raw_pc_path = os.path.join(raw_path,\"pointcloud_4m_0.25\")\n",
    "\n",
    "if not os.path.exists(raw_path): os.mkdir(raw_path)\n",
    "if not os.path.exists(raw_pc_path): os.mkdir(raw_pc_path)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "combined_point_collection = []\n",
    "\n",
    "combined_X = []\n",
    "combined_Z = []\n",
    "combined_Y = []\n",
    "\n",
    "csvfile = open(raw_path+\"/pointcloud_centroids_4m_0.25.csv\",'w',newline = '')\n",
    "\n",
    "csv_writer = csv.writer(csvfile, delimiter = ',')\n",
    "csv_writer.writerow(['timestamp', 'northing', 'easting','alting','obj'])\n",
    "\n",
    "for obj_, [object_name, pointCloud, triangles] in enumerate(new_contiguous_point_collection):\n",
    "\n",
    "    if object_name == \"Reception-Data61-L5.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) + 50\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 0\n",
    "    elif object_name == \"Driveway.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) - 25\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) - 50\n",
    "    elif object_name == \"Apartment.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) + 25\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) - 50\n",
    "    elif object_name == \"Workstations-Data61-L4.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) - 50\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 0\n",
    "    elif object_name == \"Kitchen-Data61-L4.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) + 0\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 0\n",
    "    elif object_name == \"HallWayToKitchen-Data61-L4.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) - 25\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 50\n",
    "    elif object_name == \"StairWell-Data61-L4.obj\":\n",
    "        new_X = pointCloud[:,0] - np.mean(pointCloud[:,0]) + 25\n",
    "        new_Z = pointCloud[:,2] - np.mean(pointCloud[:,2]) + 50\n",
    "\n",
    "    new_Y = pointCloud[:,1]\n",
    "\n",
    "    if len(combined_X) == 0:\n",
    "        combined_X = new_X\n",
    "        combined_Z = new_Z\n",
    "        combined_Y = new_Y\n",
    "    else:\n",
    "        combined_X = np.concatenate((combined_X,new_X),axis = 0)\n",
    "        combined_Z = np.concatenate((combined_Z,new_Z),axis = 0)\n",
    "        combined_Y = np.concatenate((combined_Y,new_Y),axis = 0)\n",
    "        \n",
    "    new_object_pointcloud = np.stack((new_X,new_Z,new_Y)).T\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=20000, algorithm='kd_tree').fit(new_object_pointcloud)\n",
    "\n",
    "    round_new_pointcloud = 0.25*100*np.around((0.01/0.25)*new_object_pointcloud,decimals=2)\n",
    "    unq_round_pointcloud = np.unique(round_new_pointcloud[:,:3],axis = 0)\n",
    "\n",
    "    raw_centroids = unq_round_pointcloud#+np.random.normal(0,0.25,unq_round_pointcloud.shape)\n",
    "\n",
    "    for northing, easting, alting in raw_centroids:\n",
    "\n",
    "        # Getting the points around our centroid defined by [northing, easting]\n",
    "        distances, indices = nbrs.kneighbors([[northing, easting, alting]])\n",
    "\n",
    "        #if max(distances[0]) < 0.5*spatial_span: continue\n",
    "\n",
    "        submap_pointcloud = new_object_pointcloud[indices[0,np.where(distances[0,:]<=spatial_span)[0]]]\n",
    "\n",
    "        if len(submap_pointcloud) == 0:\n",
    "            continue\n",
    "\n",
    "        # Centering and rescaling\n",
    "        submap_pointcloud = (submap_pointcloud - [northing, easting, alting])/spatial_span\n",
    "\n",
    "        if len(submap_pointcloud) > num_points:\n",
    "            submap_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points)]\n",
    "        elif len(submap_pointcloud) < num_points and len(submap_pointcloud) >= cutoff*num_points :\n",
    "            #print(i,submap_pointcloud.shape)\n",
    "            additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud))]\n",
    "            additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "            submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "        elif len(submap_pointcloud) < cutoff*num_points :\n",
    "            #continue\n",
    "            #print(i,submap_pointcloud.shape)\n",
    "            additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud), True)]\n",
    "            additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "            submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "\n",
    "        timestamp = int(10**16*(time.time()))\n",
    "\n",
    "\n",
    "        csv_writer.writerow([timestamp,northing,easting,alting,obj_])\n",
    "\n",
    "        with open(raw_pc_path+'/{}.pickle'.format(timestamp),'wb') as f:\n",
    "            pickle.dump(submap_pointcloud.T,f)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    print(\"Done with submap generation for object ({}) {} in {:.3f} seconds\".format(obj_,object_name,time.time()-t0))\n",
    "\n",
    "csvfile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  pointnetvlad_submaps/raw_partial_radius_0.25_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 0.25 ( 1000 samples) in 28.530 seconds\n",
      "  pointnetvlad_submaps/raw_partial_radius_0.5_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 0.5 ( 1000 samples) in 28.348 seconds\n",
      "  pointnetvlad_submaps/raw_partial_radius_0.75_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 0.75 ( 1000 samples) in 28.598 seconds\n",
      "  pointnetvlad_submaps/raw_partial_radius_1.0_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 1.0 ( 1000 samples) in 28.703 seconds\n",
      "  pointnetvlad_submaps/raw_partial_radius_1.25_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 1.25 ( 1000 samples) in 28.866 seconds\n",
      "  pointnetvlad_submaps/raw_partial_radius_1.5_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 1.5 ( 1000 samples) in 29.164 seconds\n",
      "  pointnetvlad_submaps/raw_partial_radius_1.75_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 1.75 ( 1000 samples) in 29.560 seconds\n",
      "  pointnetvlad_submaps/raw_partial_radius_2.0_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 2.0 ( 1000 samples) in 30.089 seconds\n",
      "  pointnetvlad_submaps/raw_partial_radius_2.25_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 2.25 ( 1773 samples) in 36.201 seconds\n",
      "  pointnetvlad_submaps/raw_partial_radius_2.5_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 2.5 ( 1814 samples) in 37.526 seconds\n",
      "  pointnetvlad_submaps/raw_partial_radius_2.75_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 2.75 ( 1885 samples) in 41.974 seconds\n",
      "  pointnetvlad_submaps/raw_partial_radius_3.0_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 3.0 ( 1978 samples) in 45.016 seconds\n"
     ]
    }
   ],
   "source": [
    "baseline_path = 'pointnetvlad_submaps/'\n",
    "\n",
    "if not os.path.exists(baseline_path): os.mkdir(baseline_path)\n",
    "\n",
    "spatial_span = 2.0\n",
    "\n",
    "interval = 0.5\n",
    "\n",
    "num_points = 4096\n",
    "\n",
    "partial_lengths = []\n",
    "\n",
    "cutoff = 0.5\n",
    "\n",
    "for radius in np.arange(0.25,3.1,0.25):\n",
    "    \n",
    "    per_radius_partial_length = []\n",
    "    \n",
    "    t1 = time.time()\n",
    "            \n",
    "    partial_path = os.path.join(baseline_path,\"raw_partial_radius_\"+str(radius)+\"_\"+str(num_points))+\"_unassisted\"\n",
    "    pointcloud_partial_path = os.path.join(partial_path,\"pointcloud_4m\")\n",
    "    #pointcloud_partial_bin_path = os.path.join(partial_path,\"pointcloud_4m_npy\")\n",
    "\n",
    "    if not os.path.exists(partial_path): os.mkdir(partial_path)\n",
    "    if not os.path.exists(pointcloud_partial_path): os.mkdir(pointcloud_partial_path)\n",
    "    #if not os.path.exists(pointcloud_partial_bin_path): os.mkdir(pointcloud_partial_bin_path)\n",
    "\n",
    "    print(\" \",pointcloud_partial_path)\n",
    "    #\"\"\"\n",
    "    csvfile = open(partial_path+\"/pointcloud_centroids_4m.csv\",'w',newline = '')\n",
    "\n",
    "    csv_writer = csv.writer(csvfile, delimiter = ',')\n",
    "    csv_writer.writerow(['timestamp', 'northing', 'easting', 'alting','obj'])\n",
    "    #\"\"\"\n",
    "    count = 0\n",
    "    \n",
    "    for obj_, object_name, original_vertex in sample_points:\n",
    "        \n",
    "        new_partial_pointcloud = []\n",
    "        new_vX = []\n",
    "        new_vZ = []\n",
    "        \n",
    "        try:\n",
    "\n",
    "            object_, ransac_pointCloud, tri_ = new_contiguous_point_collection[int(obj_)]\n",
    "            \n",
    "            ransac_nbrs = NearestNeighbors(n_neighbors=min(20000,len(ransac_pointCloud)), algorithm='kd_tree').fit(ransac_pointCloud[:,:3])\n",
    "            \n",
    "            dist_, ind_ = ransac_nbrs.kneighbors([original_vertex[:3]])\n",
    "            pointCloud =  ransac_pointCloud[ind_[0,np.where(dist_[0,:]<=radius)[0]]]\n",
    "        except:\n",
    "            print(\"Can't get ransac samples for\",trial,obj_meta[0],dist_.shape,ind_.shape)\n",
    "            continue\n",
    "            \n",
    "        #if len(gen_planes) == 0: continue\n",
    "        if len(pointCloud) == 0: continue\n",
    "\n",
    "        if object_name == \"Reception-Data61-L5.obj\":\n",
    "            new_X = pointCloud[:,0] + 50\n",
    "            new_Z = pointCloud[:,2] + 0\n",
    "            new_vX = original_vertex[0] + 50\n",
    "            new_vZ = original_vertex[2] + 0\n",
    "        elif object_name == \"Driveway.obj\":\n",
    "            new_X = pointCloud[:,0] - 25\n",
    "            new_Z = pointCloud[:,2] - 50\n",
    "            new_vX = original_vertex[0] - 25\n",
    "            new_vZ = original_vertex[2] - 50\n",
    "        elif object_name == \"Apartment.obj\":\n",
    "            new_X = pointCloud[:,0] + 25\n",
    "            new_Z = pointCloud[:,2] - 50\n",
    "            new_vX = original_vertex[0] + 25\n",
    "            new_vZ = original_vertex[2] - 50\n",
    "        elif object_name == \"Workstations-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] - 50\n",
    "            new_Z = pointCloud[:,2] + 0\n",
    "            new_vX = original_vertex[0] - 50\n",
    "            new_vZ = original_vertex[2] + 0\n",
    "        elif object_name == \"Kitchen-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] + 0\n",
    "            new_Z = pointCloud[:,2] + 0\n",
    "            new_vX = original_vertex[0] + 0\n",
    "            new_vZ = original_vertex[2] + 0\n",
    "        elif object_name == \"HallWayToKitchen-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] - 25\n",
    "            new_Z = pointCloud[:,2] + 50\n",
    "            new_vX = original_vertex[0] - 25\n",
    "            new_vZ = original_vertex[2] + 50\n",
    "        elif object_name == \"StairWell-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] + 25\n",
    "            new_Z = pointCloud[:,2] + 50\n",
    "            new_vX = original_vertex[0] + 25\n",
    "            new_vZ = original_vertex[2] + 50\n",
    "        else:\n",
    "            print(\"Error:\",obj_meta)\n",
    "\n",
    "        new_Y = pointCloud[:,1]\n",
    "\n",
    "        new_partial_pointcloud = np.stack((new_X,new_Z,new_Y)).T\n",
    "        \n",
    "        max_known_span = max(np.amax(new_partial_pointcloud, axis = 0) - np.amin(new_partial_pointcloud, axis = 0))\n",
    "        \n",
    "        nbrs = NearestNeighbors(n_neighbors=min(2*num_points,len(new_partial_pointcloud)), algorithm='kd_tree').fit(new_partial_pointcloud)\n",
    "\n",
    "        # Get submap \"centroids\" by quantizing by 0.25m, i.e. round then unique\n",
    "        if max_known_span > 2*spatial_span:\n",
    "            round_new_partial_pointcloud = 100*np.around(0.01*new_partial_pointcloud,decimals=2)\n",
    "            unq_round_partial_pointcloud = np.unique(round_new_partial_pointcloud[:,:3],axis = 0)\n",
    "        #    \n",
    "            raw_partial_centroids = unq_round_partial_pointcloud\n",
    "            c_nbrs = NearestNeighbors(n_neighbors = min(25,len(raw_partial_centroids)),  algorithm='kd_tree').fit(raw_partial_centroids)\n",
    "            c_dist, c_ind = c_nbrs.kneighbors(raw_partial_centroids)\n",
    "\n",
    "            ia1, ia2 = np.where(c_dist < 1.73)\n",
    "            \n",
    "\n",
    "            dist_bins = np.bincount(ia1)\n",
    "            max_dist = max(np.bincount(ia1))\n",
    "            raw_partial_centroids = raw_partial_centroids[[i for i, j in enumerate(dist_bins) if j == max_dist]]\n",
    "            \n",
    "            raw_partial_centroids = raw_partial_centroids+np.random.normal(0,interval,raw_partial_centroids.shape)\n",
    "            \n",
    "        else:\n",
    "            # Correcting this, because the attacker is supposed to not know the true centroid\n",
    "            # and has to estimate it instead.\n",
    "            #raw_partial_centroids = [[new_vX, new_vZ, original_vertex[1]]]        \n",
    "\n",
    "            raw_partial_centroids = [np.mean(new_partial_pointcloud, axis = 0)]\n",
    "            \n",
    "        for northing, easting, alting in raw_partial_centroids:\n",
    "            \n",
    "            # Getting the points around our centroid defined by [northing, easting]\n",
    "            distances, indices = nbrs.kneighbors([[northing, easting, alting]])\n",
    "            \n",
    "            #if max(distances[0]) < 0.5*spatial_span: continue\n",
    "                \n",
    "            submap_pointcloud = new_partial_pointcloud[indices[0,np.where(distances[0,:]<=spatial_span)[0]]]\n",
    "\n",
    "            if len(submap_pointcloud) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Centering and rescaling\n",
    "            submap_pointcloud = (submap_pointcloud - [northing, easting, alting])/spatial_span\n",
    "            per_radius_partial_length.append([northing, easting, alting, len(submap_pointcloud)])\n",
    "\n",
    "            if len(submap_pointcloud) > num_points:\n",
    "                submap_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points)]\n",
    "            elif len(submap_pointcloud) < num_points and len(submap_pointcloud) >= cutoff*num_points :\n",
    "                #print(i,submap_pointcloud.shape)\n",
    "                additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud))]\n",
    "                additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "                submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "            elif len(submap_pointcloud) < cutoff*num_points :\n",
    "                #continue\n",
    "                #print(i,submap_pointcloud.shape)\n",
    "                additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud), True)]\n",
    "                additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "                submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "\n",
    "            timestamp = int(10**16*(time.time()))\n",
    "            \n",
    "\n",
    "            csv_writer.writerow([timestamp,northing,easting,alting,obj_])\n",
    "            \n",
    "            with open(pointcloud_partial_path+'/{}.pickle'.format(timestamp),'wb') as f:\n",
    "                pickle.dump(submap_pointcloud.T,f)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            \"\"\"\n",
    "            with open(\"pnvlad/pointcloud_4m_0.25overlap/raw_{:05d}.bin\".format(i),'wb') as byteFile:\n",
    "                byteFile.write(bytes(np.asarray(submap_pointcloud.T,dtype=np.double)))\n",
    "\n",
    "            \"\"\"\n",
    "            \n",
    "    partial_lengths.append([\n",
    "        radius,\n",
    "        per_radius_partial_length\n",
    "    ])\n",
    "\n",
    "    print(\"   Done with submap generation for radius {} ( {} samples) in {:.3f} seconds\".format(radius,count,time.time()-t1))\n",
    "    #print(len(partial_lengths))\n",
    "    csvfile.close()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  pointnetvlad_submaps/ransac_partial_radius_0.25_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 0.25 ( 990 samples) in 39.245 seconds\n",
      "  pointnetvlad_submaps/ransac_partial_radius_0.5_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 0.5 ( 992 samples) in 38.529 seconds\n",
      "  pointnetvlad_submaps/ransac_partial_radius_0.75_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 0.75 ( 997 samples) in 37.563 seconds\n",
      "  pointnetvlad_submaps/ransac_partial_radius_1.0_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 1.0 ( 997 samples) in 37.492 seconds\n",
      "  pointnetvlad_submaps/ransac_partial_radius_1.25_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 1.25 ( 998 samples) in 37.667 seconds\n",
      "  pointnetvlad_submaps/ransac_partial_radius_1.5_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 1.5 ( 1000 samples) in 37.727 seconds\n",
      "  pointnetvlad_submaps/ransac_partial_radius_1.75_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 1.75 ( 1000 samples) in 39.146 seconds\n",
      "  pointnetvlad_submaps/ransac_partial_radius_2.0_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 2.0 ( 1000 samples) in 38.864 seconds\n",
      "  pointnetvlad_submaps/ransac_partial_radius_2.25_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 2.25 ( 1776 samples) in 44.993 seconds\n",
      "  pointnetvlad_submaps/ransac_partial_radius_2.5_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 2.5 ( 1848 samples) in 47.565 seconds\n",
      "  pointnetvlad_submaps/ransac_partial_radius_2.75_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 2.75 ( 1929 samples) in 51.556 seconds\n",
      "  pointnetvlad_submaps/ransac_partial_radius_3.0_4096_unassisted/pointcloud_4m\n",
      "   Done with submap generation for radius 3.0 ( 2016 samples) in 53.975 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "baseline_path = 'pointnetvlad_submaps/'\n",
    "\n",
    "if not os.path.exists(baseline_path): os.mkdir(baseline_path)\n",
    "\n",
    "spatial_span = 2.0\n",
    "\n",
    "interval = 0.5\n",
    "\n",
    "num_points = 4096\n",
    "\n",
    "ransac_partial_lengths = []\n",
    "\n",
    "cutoff = 0.5\n",
    "\n",
    "for radius in np.arange(0.25,3.1,0.25):\n",
    "    \n",
    "    per_radius_partial_length = []\n",
    "    \n",
    "    t1 = time.time()\n",
    "            \n",
    "    partial_path = os.path.join(baseline_path,\"ransac_partial_radius_\"+str(radius)+\"_\"+str(num_points))+\"_unassisted\"\n",
    "    pointcloud_partial_path = os.path.join(partial_path,\"pointcloud_4m\")\n",
    "    #pointcloud_partial_bin_path = os.path.join(partial_path,\"pointcloud_4m_npy\")\n",
    "\n",
    "    if not os.path.exists(partial_path): os.mkdir(partial_path)\n",
    "    if not os.path.exists(pointcloud_partial_path): os.mkdir(pointcloud_partial_path)\n",
    "    #if not os.path.exists(pointcloud_partial_bin_path): os.mkdir(pointcloud_partial_bin_path)\n",
    "\n",
    "    print(\" \",pointcloud_partial_path)\n",
    "    #\"\"\"\n",
    "    csvfile = open(partial_path+\"/pointcloud_centroids_4m.csv\",'w',newline = '')\n",
    "\n",
    "    csv_writer = csv.writer(csvfile, delimiter = ',')\n",
    "    csv_writer.writerow(['timestamp', 'northing', 'easting', 'alting','obj'])\n",
    "    #\"\"\"\n",
    "    count = 0\n",
    "    \n",
    "    for obj_, object_name, original_vertex in sample_points:\n",
    "        \n",
    "        new_partial_pointcloud = []\n",
    "        new_vX = []\n",
    "        new_vZ = []\n",
    "        \n",
    "        try:\n",
    "            trial = np.random.randint(5)\n",
    "            \n",
    "            with open(\"../ransac_pc/ransac_point_collection_{}.pickle\".format(trial),'rb') as f:\n",
    "                ransac_trial_point_collection = pickle.load(f)\n",
    "\n",
    "            object_, ransac_pointCloud, tri_ = ransac_trial_point_collection[int(obj_)]\n",
    "            \n",
    "            ransac_nbrs = NearestNeighbors(n_neighbors=min(20000,len(ransac_pointCloud)), algorithm='kd_tree').fit(ransac_pointCloud[:,:3])\n",
    "            \n",
    "            dist_, ind_ = ransac_nbrs.kneighbors([original_vertex[:3]])\n",
    "            pointCloud =  ransac_pointCloud[ind_[0,np.where(dist_[0,:]<=radius)[0]]]\n",
    "        except:\n",
    "            print(\"Can't get ransac samples for\",trial,obj_meta[0])\n",
    "            continue\n",
    "            \n",
    "        #if len(gen_planes) == 0: continue\n",
    "        if len(pointCloud) == 0: continue\n",
    "\n",
    "        if object_name == \"Reception-Data61-L5.obj\":\n",
    "            new_X = pointCloud[:,0] + 50\n",
    "            new_Z = pointCloud[:,2] + 0\n",
    "            new_vX = original_vertex[0] + 50\n",
    "            new_vZ = original_vertex[2] + 0\n",
    "        elif object_name == \"Driveway.obj\":\n",
    "            new_X = pointCloud[:,0] - 25\n",
    "            new_Z = pointCloud[:,2] - 50\n",
    "            new_vX = original_vertex[0] - 25\n",
    "            new_vZ = original_vertex[2] - 50\n",
    "        elif object_name == \"Apartment.obj\":\n",
    "            new_X = pointCloud[:,0] + 25\n",
    "            new_Z = pointCloud[:,2] - 50\n",
    "            new_vX = original_vertex[0] + 25\n",
    "            new_vZ = original_vertex[2] - 50\n",
    "        elif object_name == \"Workstations-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] - 50\n",
    "            new_Z = pointCloud[:,2] + 0\n",
    "            new_vX = original_vertex[0] - 50\n",
    "            new_vZ = original_vertex[2] + 0\n",
    "        elif object_name == \"Kitchen-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] + 0\n",
    "            new_Z = pointCloud[:,2] + 0\n",
    "            new_vX = original_vertex[0] + 0\n",
    "            new_vZ = original_vertex[2] + 0\n",
    "        elif object_name == \"HallWayToKitchen-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] - 25\n",
    "            new_Z = pointCloud[:,2] + 50\n",
    "            new_vX = original_vertex[0] - 25\n",
    "            new_vZ = original_vertex[2] + 50\n",
    "        elif object_name == \"StairWell-Data61-L4.obj\":\n",
    "            new_X = pointCloud[:,0] + 25\n",
    "            new_Z = pointCloud[:,2] + 50\n",
    "            new_vX = original_vertex[0] + 25\n",
    "            new_vZ = original_vertex[2] + 50\n",
    "        else:\n",
    "            print(\"Error:\",obj_meta)\n",
    "\n",
    "        new_Y = pointCloud[:,1]\n",
    "\n",
    "        new_partial_pointcloud = np.stack((new_X,new_Z,new_Y)).T\n",
    "        \n",
    "        max_known_span = max(np.amax(new_partial_pointcloud, axis = 0) - np.amin(new_partial_pointcloud, axis = 0))\n",
    "        \n",
    "        nbrs = NearestNeighbors(n_neighbors=min(2*num_points,len(new_partial_pointcloud)), algorithm='kd_tree').fit(new_partial_pointcloud)\n",
    "\n",
    "        # Get submap \"centroids\" by quantizing by 0.25m, i.e. round then unique\n",
    "        if max_known_span > 2*spatial_span:\n",
    "            round_new_partial_pointcloud = 100*np.around(0.01*new_partial_pointcloud,decimals=2)\n",
    "            unq_round_partial_pointcloud = np.unique(round_new_partial_pointcloud[:,:3],axis = 0)\n",
    "        #    \n",
    "            raw_partial_centroids = unq_round_partial_pointcloud\n",
    "            c_nbrs = NearestNeighbors(n_neighbors = min(25,len(raw_partial_centroids)),  algorithm='kd_tree').fit(raw_partial_centroids)\n",
    "            c_dist, c_ind = c_nbrs.kneighbors(raw_partial_centroids)\n",
    "\n",
    "            ia1, ia2 = np.where(c_dist < 1.73)\n",
    "            \n",
    "\n",
    "            dist_bins = np.bincount(ia1)\n",
    "            max_dist = max(np.bincount(ia1))\n",
    "            raw_partial_centroids = raw_partial_centroids[[i for i, j in enumerate(dist_bins) if j == max_dist]]\n",
    "            \n",
    "            raw_partial_centroids = raw_partial_centroids+np.random.normal(0,interval,raw_partial_centroids.shape)\n",
    "            \n",
    "        else:\n",
    "            # Correcting this, because the attacker is supposed to not know the true centroid\n",
    "            # and has to estimate it instead.\n",
    "            #raw_partial_centroids = [[new_vX, new_vZ, original_vertex[1]]]        \n",
    "\n",
    "            raw_partial_centroids = [np.mean(new_partial_pointcloud, axis = 0)]\n",
    "            \n",
    "        for northing, easting, alting in raw_partial_centroids:\n",
    "            \n",
    "            # Getting the points around our centroid defined by [northing, easting]\n",
    "            distances, indices = nbrs.kneighbors([[northing, easting, alting]])\n",
    "            \n",
    "            #if max(distances[0]) < 0.5*spatial_span: continue\n",
    "                \n",
    "            submap_pointcloud = new_partial_pointcloud[indices[0,np.where(distances[0,:]<=spatial_span)[0]]]\n",
    "\n",
    "            if len(submap_pointcloud) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Centering and rescaling\n",
    "            submap_pointcloud = (submap_pointcloud - [northing, easting, alting])/spatial_span\n",
    "            per_radius_partial_length.append([northing, easting, alting, len(submap_pointcloud)])\n",
    "\n",
    "            if len(submap_pointcloud) > num_points:\n",
    "                submap_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points)]\n",
    "            elif len(submap_pointcloud) < num_points and len(submap_pointcloud) >= cutoff*num_points :\n",
    "                #print(i,submap_pointcloud.shape)\n",
    "                additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud))]\n",
    "                additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "                submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "            elif len(submap_pointcloud) < cutoff*num_points :\n",
    "                #continue\n",
    "                #print(i,submap_pointcloud.shape)\n",
    "                additional_pointcloud = submap_pointcloud[np.random.choice(len(submap_pointcloud),num_points-len(submap_pointcloud), True)]\n",
    "                additional_pointcloud = additional_pointcloud + np.random.normal(0,0.05,additional_pointcloud.shape)\n",
    "                submap_pointcloud = np.concatenate((submap_pointcloud,additional_pointcloud),axis = 0)\n",
    "\n",
    "            timestamp = int(10**16*(time.time()))\n",
    "            \n",
    "\n",
    "            csv_writer.writerow([timestamp,northing,easting,alting,obj_])\n",
    "            \n",
    "            with open(pointcloud_partial_path+'/{}.pickle'.format(timestamp),'wb') as f:\n",
    "                pickle.dump(submap_pointcloud.T,f)\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            \"\"\"\n",
    "            with open(\"pnvlad/pointcloud_4m_0.25overlap/raw_{:05d}.bin\".format(i),'wb') as byteFile:\n",
    "                byteFile.write(bytes(np.asarray(submap_pointcloud.T,dtype=np.double)))\n",
    "\n",
    "            \"\"\"\n",
    "            \n",
    "    ransac_partial_lengths.append([\n",
    "        radius,\n",
    "        per_radius_partial_length\n",
    "    ])\n",
    "\n",
    "    print(\"   Done with submap generation for radius {} ( {} samples) in {:.3f} seconds\".format(radius,count,time.time()-t1))\n",
    "    #print(len(partial_lengths))\n",
    "    csvfile.close()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_in_test_set(northing, easting, points, x_width, y_width):\n",
    "\tin_test_set=False\n",
    "\tfor point in points:\n",
    "\t\tif(point[0]-x_width<northing and northing< point[0]+x_width and point[1]-y_width<easting and easting<point[1]+y_width):\n",
    "\t\t\tin_test_set=True\n",
    "\t\t\tbreak\n",
    "\treturn in_test_set\n",
    "##########################################\n",
    "\n",
    "def output_to_file(output, filename):\n",
    "\twith open(filename, 'wb') as handle:\n",
    "\t    pickle.dump(output, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\tprint(\"Done \", filename)\n",
    "    \n",
    "def get_sets_dict(filename):\n",
    "\t#[key_dataset:{key_pointcloud:{'query':file,'northing':value,'easting':value}},key_dataset:{key_pointcloud:{'query':file,'northing':value,'easting':value}}, ...}\n",
    "\twith open(filename, 'rb') as handle:\n",
    "\t\ttrajectories = pickle.load(handle)\n",
    "\t\tprint(\"Database Trajectories Loaded.\")\n",
    "\t\treturn trajectories\n",
    "    \n",
    "def construct_query_and_database_sets(base_path, folders, pointcloud_fols, filename):#, partial_name):#, p, output_name):\n",
    "\tdatabase_trees=[]\n",
    "\ttest_trees=[]\n",
    "\tfor folder in folders:\n",
    "\t\tprint(folder)\n",
    "\t\tdf_database= pd.DataFrame(columns=['file','northing','easting','alting','obj'])\n",
    "\t\t#df_test= pd.DataFrame(columns=['file','northing','easting','alting'])\n",
    "\t\t\n",
    "\t\tdf_locations= pd.read_csv(os.path.join(base_path,folder,filename),sep=',')\n",
    "\t\t#df_locations['timestamp']=folder+pointcloud_fols+df_locations['timestamp'].astype(str)+'.bin'\n",
    "\t\t#df_locations=df_locations.rename(columns={'timestamp':'file'})\n",
    "\t\tfor index, row in df_locations.iterrows():\n",
    "\t\t\t#entire business district is in the test set\n",
    "\t\t\t#if folder not in train_folders:\n",
    "\t\t\t#\t#print(\"test\",folder)\n",
    "\t\t\t#\tdf_test=df_test.append(row, ignore_index=True)\n",
    "\t\t\t#elif(check_in_test_set(row['northing'], row['easting'], p, x_width, y_width)):\n",
    "\t\t\t#df_test=df_test.append(row, ignore_index=True)\n",
    "\t\t\tdf_database=df_database.append(row, ignore_index=True)\n",
    "\n",
    "\t\tdatabase_tree = KDTree(df_database[['northing','easting','alting','obj']])\n",
    "\t\tdatabase_trees.append(database_tree)\n",
    "\n",
    "\ttest_sets=[]\n",
    "\tdatabase_sets=[]\n",
    "\tfor folder in folders:\n",
    "\t\tdatabase={}\n",
    "\t\ttest={} \n",
    "\t\tdf_locations= pd.read_csv(os.path.join(base_path,folder,filename),sep=',')\n",
    "\t\tdf_locations['timestamp']=folder+pointcloud_fols+df_locations['timestamp'].astype(str)+'.pickle'\n",
    "\t\tdf_locations=df_locations.rename(columns={'timestamp':'file'})\n",
    "\t\tfor index,row in df_locations.iterrows():\t\t\t\t\n",
    "\t\t\t#if folder not in train_folders:\n",
    "\t\t\t#\t#print(\"test\",folder)\n",
    "\t\t\t#\ttest[len(test.keys())]={'query':row['file'],'northing':row['northing'],'easting':row['easting'],'alting':row['alting']}\n",
    "\t\t\t#elif(check_in_test_set(row['northing'], row['easting'], p, x_width, y_width)):\n",
    "\t\t\t#test[len(test.keys())]={'query':row['file'],'northing':row['northing'],'easting':row['easting']}\n",
    "\t\t\tdatabase[len(database.keys())]={'query':row['file'],'northing':row['northing'],'easting':row['easting'],'alting':row['alting']}\n",
    "\t\tdatabase_sets.append(database)\n",
    "\t\t#if folder not in train_folders:\n",
    "\t\t#\ttest_sets.append(test)\n",
    "            \n",
    "\tprint(\"Database (Tree) sets:\",len(database_sets))    \n",
    "\n",
    "\toutput_to_file(database_sets, 'pointnetvlad_submaps/3d_evaluation_database.pickle')\n",
    "    #'partial_spaces/'+partial_name+'_evaluation_database.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw_dataset\n",
      "Database (Tree) sets: 1\n",
      "Done  pointnetvlad_submaps/3d_evaluation_database.pickle\n"
     ]
    }
   ],
   "source": [
    "###Building database and query files for evaluation\n",
    "base_path= \"pointnetvlad_submaps/\"#\"../partial_dataset/\"\n",
    "\n",
    "#\t\n",
    "construct_query_and_database_sets(\n",
    "    base_path, \n",
    "    ['raw_dataset'], \n",
    "    \"/pointcloud_4m_0.25/\",\n",
    "    \"pointcloud_centroids_4m_0.25.csv\")#, all_folders[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_query_sets(partial_path, pointcloud_fols, filename):#, partial_name):#, p, output_name):\n",
    "\ttest_trees=[]\n",
    "    \n",
    "\t#for folder in folders:\n",
    "\t#\tprint(folder)\n",
    "\tdf_test= pd.DataFrame(columns=['file','northing','easting','alting','obj'])\n",
    "        \n",
    "\tdf_locations= pd.read_csv(os.path.join(base_path,partial_path,filename),sep=',')\n",
    "\t#df_locations['timestamp']=folder+pointcloud_fols+df_locations['timestamp'].astype(str)+'.bin'\n",
    "\t#df_locations=df_locations.rename(columns={'timestamp':'file'})\n",
    "\tfor index, row in df_locations.iterrows():\n",
    "\t\tdf_test=df_test.append(row, ignore_index=True)\n",
    "\t\t#elif(check_in_test_set(row['northing'], row['easting'], p, x_width, y_width)):\n",
    "\t\t#df_test=df_test.append(row, ignore_index=True)\n",
    "\ttest_tree = KDTree(df_test[['northing','easting','alting','obj']])\n",
    "\ttest_trees.append(test_tree)\n",
    "\n",
    "\ttest_sets=[]\n",
    "\t#for folder in folders:\n",
    "\ttest={} \n",
    "\tdf_locations['timestamp']=partial_path+pointcloud_fols+df_locations['timestamp'].astype(str)+'.pickle'\n",
    "\tdf_locations=df_locations.rename(columns={'timestamp':'file'})\n",
    "\tfor index,row in df_locations.iterrows():\t\t\t\t\n",
    "\t\t#entire business district is in the test set\n",
    "\t\ttest[len(test.keys())]={'query':row['file'],'northing':row['northing'],'easting':row['easting'],'alting':row['alting']}\n",
    "\t\t#elif(check_in_test_set(row['northing'], row['easting'], p, x_width, y_width)):\n",
    "\t\t#test[len(test.keys())]={'query':row['file'],'northing':row['northing'],'easting':row['easting']}\n",
    "\ttest_sets.append(test)\n",
    "            \n",
    "\tprint(\" Test (Tree) sets:\",len(test_sets))    \n",
    "\n",
    "\tfor i in range(len(database_sets)):\n",
    "\t\ttree=database_trees[i]\n",
    "\t\tfor j in range(len(test_sets)):\n",
    "\t\t\t#if(i==j):\n",
    "\t\t\t#\tcontinue\n",
    "\t\t\tfor key in range(len(test_sets[j].keys())):\n",
    "\t\t\t\tcoor=np.array([[test_sets[j][key][\"northing\"],test_sets[j][key][\"easting\"],test_sets[j][key][\"alting\"]]])\n",
    "\t\t\t\tindex = tree.query_radius(coor, r=20) #r=4\n",
    "\t\t\t\t#indices of the positive matches in database i of each query (key) in test set j\n",
    "\t\t\t\ttest_sets[j][key][i]=index[0].tolist()\n",
    "\n",
    "\t#'partial_spaces/'+partial_name+'_evaluation_database.pickle')\n",
    "\toutput_to_file(test_sets, base_path+'3d_{}_evaluation_query.pickle'.format(partial_path))#'partial_spaces/'+partial_name+'_evaluation_query.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Trajectories Loaded.\n"
     ]
    }
   ],
   "source": [
    "database_sets = get_sets_dict('pointnetvlad_submaps/3d_evaluation_database.pickle')\n",
    "\n",
    "database_trees=[]\n",
    "\n",
    "df_database= pd.DataFrame(columns=['file','northing','easting','alting'])\n",
    "\n",
    "df_locations= pd.read_csv(os.path.join(base_path,'raw_dataset',\"pointcloud_centroids_4m_0.25.csv\"),sep=',')\n",
    "#df_locations['timestamp']=folder+pointcloud_fols+df_locations['timestamp'].astype(str)+'.bin'\n",
    "#df_locations=df_locations.rename(columns={'timestamp':'file'})\n",
    "for index, row in df_locations.iterrows():\n",
    "    #df_test=df_test.append(row, ignore_index=True)\n",
    "    df_database=df_database.append(row, ignore_index=True)\n",
    "\n",
    "database_tree = KDTree(df_database[['northing','easting','alting']])\n",
    "database_trees.append(database_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ransac_partial_radius_0.25_4096_unassisted\n",
      " Test (Tree) sets: 1\n",
      "Done  pointnetvlad_submaps/3d_ransac_partial_radius_0.25_4096_unassisted_evaluation_query.pickle\n",
      "ransac_partial_radius_0.5_4096_unassisted\n",
      " Test (Tree) sets: 1\n",
      "Done  pointnetvlad_submaps/3d_ransac_partial_radius_0.5_4096_unassisted_evaluation_query.pickle\n",
      "ransac_partial_radius_0.75_4096_unassisted\n",
      " Test (Tree) sets: 1\n",
      "Done  pointnetvlad_submaps/3d_ransac_partial_radius_0.75_4096_unassisted_evaluation_query.pickle\n",
      "ransac_partial_radius_1.0_4096_unassisted\n",
      " Test (Tree) sets: 1\n",
      "Done  pointnetvlad_submaps/3d_ransac_partial_radius_1.0_4096_unassisted_evaluation_query.pickle\n",
      "ransac_partial_radius_1.25_4096_unassisted\n",
      " Test (Tree) sets: 1\n",
      "Done  pointnetvlad_submaps/3d_ransac_partial_radius_1.25_4096_unassisted_evaluation_query.pickle\n",
      "ransac_partial_radius_1.5_4096_unassisted\n",
      " Test (Tree) sets: 1\n",
      "Done  pointnetvlad_submaps/3d_ransac_partial_radius_1.5_4096_unassisted_evaluation_query.pickle\n",
      "ransac_partial_radius_1.75_4096_unassisted\n",
      " Test (Tree) sets: 1\n",
      "Done  pointnetvlad_submaps/3d_ransac_partial_radius_1.75_4096_unassisted_evaluation_query.pickle\n",
      "ransac_partial_radius_2.0_4096_unassisted\n",
      " Test (Tree) sets: 1\n",
      "Done  pointnetvlad_submaps/3d_ransac_partial_radius_2.0_4096_unassisted_evaluation_query.pickle\n",
      "ransac_partial_radius_2.25_4096_unassisted\n",
      " Test (Tree) sets: 1\n",
      "Done  pointnetvlad_submaps/3d_ransac_partial_radius_2.25_4096_unassisted_evaluation_query.pickle\n",
      "ransac_partial_radius_2.5_4096_unassisted\n",
      " Test (Tree) sets: 1\n",
      "Done  pointnetvlad_submaps/3d_ransac_partial_radius_2.5_4096_unassisted_evaluation_query.pickle\n",
      "ransac_partial_radius_2.75_4096_unassisted\n",
      " Test (Tree) sets: 1\n",
      "Done  pointnetvlad_submaps/3d_ransac_partial_radius_2.75_4096_unassisted_evaluation_query.pickle\n",
      "ransac_partial_radius_3.0_4096_unassisted\n",
      " Test (Tree) sets: 1\n",
      "Done  pointnetvlad_submaps/3d_ransac_partial_radius_3.0_4096_unassisted_evaluation_query.pickle\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for radius in np.arange(0.25,3.1,0.25):\n",
    "    \n",
    "    partial_path = 'ransac_partial_radius_'+str(radius)+\"_4096_unassisted\"#\n",
    "    \n",
    "    print(partial_path)\n",
    "    construct_query_sets(partial_path, \"/pointcloud_4m/\", \"pointcloud_centroids_4m.csv\")#, all_folders[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-3d_env] *",
   "language": "python",
   "name": "conda-env-.conda-3d_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
