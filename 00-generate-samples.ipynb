{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import time\n",
    "import h5py\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "\n",
    "from numpy import linalg as LA\n",
    "from scipy.spatial import Delaunay\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "#sys.path.insert(0, \"../\")\n",
    "from info3d import *\n",
    "from nn_matchers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Global parameters\n",
    "\n",
    "radius_range = np.arange(0.5,1.6,0.5)\n",
    "\n",
    "with open('point_collection/new_contiguous_point_collection.pickle','rb') as f: \n",
    "    new_contiguous_point_collection = pickle.load(f)\n",
    "\n",
    "point_collection_indices = np.arange(len(new_contiguous_point_collection))\n",
    "point_collection_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 Getting sample points for one-time partial radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating 1000 samples in 0.123 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([153., 138., 141., 137., 136., 150., 145.]),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7]),\n",
       " <a list of 7 Patch objects>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "samples = 1000\n",
    "\n",
    "sample_points = []\n",
    "\n",
    "samples_indeces = []\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for i in np.arange(samples):\n",
    "\n",
    "    random_object = np.random.choice(point_collection_indices)\n",
    "\n",
    "    object_name = new_contiguous_point_collection[random_object][0]\n",
    "    pointCloud = new_contiguous_point_collection[random_object][1]\n",
    "    triangles = new_contiguous_point_collection[random_object][2]\n",
    "    \n",
    "    triangle_index = np.random.choice(np.arange(len(triangles)))\n",
    "    vertex_index = triangles[triangle_index,1]\n",
    "    original_vertex = pointCloud[vertex_index]\n",
    "\n",
    "    sample_points.append([\n",
    "        random_object, \n",
    "        object_name, \n",
    "        original_vertex\n",
    "    ])\n",
    "    \n",
    "    samples_indeces.append(random_object)\n",
    "    \n",
    "print(\"Done generating\",len(sample_points),\"samples in {:.3f} seconds.\".format(time.time()-t0))\n",
    "\n",
    "with open('sample_points.pickle','wb') as f:\n",
    "    pickle.dump(sample_points,f)\n",
    "    \n",
    "plt.title(\"Distribution of the sample spaces\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Sample space\")\n",
    "plt.hist(samples_indeces,bins = np.arange(0,8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        \n",
    "def GetPartialPointCloudOptimizedNearby(\n",
    "    pointCloud,\n",
    "    triangles,\n",
    "    radius = 1,\n",
    "    vertex = [],\n",
    "    verbose = False,\n",
    "    extract = False,\n",
    "    pointCloud_index = 0,\n",
    "    nearby = False,\n",
    "    nearby_threshold = 1.0\n",
    "):\n",
    "\n",
    "    t0 = time.time()\n",
    "    triangle_indices = np.arange(len(triangles))\n",
    "    \n",
    "    if vertex == []:\n",
    "        get_new_triangle = np.random.choice(triangle_indices)\n",
    "    #rint(\"origin-triangle index\",get_new_triangle,\"(Remember, the triangle indices can be more than the point population.)\")\n",
    "        vertex = triangles[get_new_triangle,1]\n",
    "        if verbose: print(\" Computed origin-vertex\",vertex)\n",
    "    \n",
    "    vertex = np.clip(vertex,0,len(pointCloud))\n",
    "\n",
    "    if extract:\n",
    "        nbrs = NearestNeighbors(n_neighbors=len(pointCloud)-1, algorithm='brute').fit(pointCloud[:,:3])\n",
    "        distances, indices = nbrs.kneighbors(pointCloud[:,:3])\n",
    "    \n",
    "    distances = extracted_nns[pointCloud_index][1]\n",
    "    indices = extracted_nns[pointCloud_index][2]\n",
    "    \n",
    "    if nearby:    \n",
    "        local_indexs =  indices[vertex,np.where(distances[vertex]<(radius*nearby_threshold))[0]]\n",
    "        nearby_vertex = np.random.choice(local_indexs)\n",
    "        vertex = nearby_vertex\n",
    "        \n",
    "    original_vertex = pointCloud[vertex]\n",
    "    if verbose: print(\"   \",original_vertex[:3],vertex,pointCloud[vertex,:3])\n",
    "    # makes sure that we don't get a point beyond the pC size\n",
    "    # 1.b\n",
    "    # list EVERYTHING, then update one-by-one\n",
    "\n",
    "    #t1 = time.time()\n",
    "    #print(\"  GetPartialPointCloud: Done getting nearest neighbors {:.3f} \".format(t1-t0))\n",
    "    \n",
    "    partial_pointcloud = []\n",
    "    partial_triangles = []\n",
    "    \n",
    "    #while len(triangle_indices)>0:\n",
    "    t0 = time.time()\n",
    "    # Get a starting vertex\n",
    "    \n",
    "\n",
    "    # Get the acceptable neighbors of the chosen point\n",
    "    acceptable_point_neighbors = indices[vertex,np.where(distances[vertex]<radius)[0]]\n",
    "    depletable_triangles = np.copy(triangles)\n",
    "    #print(len(acceptable_point_neighbors),\"neighbors\")\n",
    "    acceptable_neighbor_distances = distances[vertex,np.where(distances[vertex]<radius)[0]]\n",
    "    \n",
    "    #t2 = time.time()\n",
    "    #print(\"  GetPartialPointCloud: Done getting acceptable neighbors {:.3f} \".format(t2-t1))\n",
    "\n",
    "    # While distance of points to be addded are less than 1,\n",
    "    # get the indices of the connected items.\n",
    "    prev_length = 0\n",
    "    stopcount = 0\n",
    "\n",
    "    # Get all triangles with the index in the neighbor list\n",
    "    for v in acceptable_point_neighbors:    \n",
    "        included_triangle_indices = np.concatenate((\n",
    "            np.where(depletable_triangles[:,0]==v)[0],\n",
    "            np.where(depletable_triangles[:,1]==v)[0],\n",
    "            np.where(depletable_triangles[:,2]==v)[0]),0)\n",
    "\n",
    "        local_triangles = depletable_triangles[np.unique(included_triangle_indices)]\n",
    "        depletable_triangles = np.delete(depletable_triangles,np.unique(included_triangle_indices),0)\n",
    "\n",
    "        if len(partial_triangles) > 0:\n",
    "            partial_triangles = np.concatenate((partial_triangles,local_triangles),0)\n",
    "            partial_triangles = np.asarray(partial_triangles)\n",
    "        else:\n",
    "            partial_triangles = local_triangles\n",
    "\n",
    "    included_vertices = np.unique(partial_triangles.flatten('C'))\n",
    "    index_list = []\n",
    "    \n",
    "    #t3 = time.time()\n",
    "    #print(\"  GetPartialPointCloud: Done getting vertices {:.3f} \".format(t3 - t2))\n",
    "\n",
    "    for in_vertex in included_vertices:\n",
    "        if in_vertex in acceptable_point_neighbors:\n",
    "            # before adding to the partial lists, check if in the acceptable neighbor list.\n",
    "\n",
    "            partial_pointcloud.append(pointCloud[in_vertex])\n",
    "            #depletable_neighbors = np.delete(depletable_neighbors,np.where(depletable_neighbors==in_vertex),0)\n",
    "            index_list.append(in_vertex)\n",
    "            np.place(partial_triangles,partial_triangles==in_vertex,len(partial_pointcloud)-1)\n",
    "        else:\n",
    "            # if not, remove associated triangles\n",
    "            partial_triangles = np.delete(partial_triangles,np.unique(np.where(partial_triangles==in_vertex)[0]),0)\n",
    "\n",
    "    #t4 = time.time()\n",
    "    #print(\"  GetPartialPointCloud: Done getting triangles {:.3f} \".format(t4 - t3))\n",
    "    \n",
    "    partial_pointcloud = np.asarray(partial_pointcloud)\n",
    "   \n",
    "    return partial_pointcloud, partial_triangles, original_vertex, vertex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Compute the descriptors from the extracted point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "descriptors = []\n",
    "\n",
    "for object_name, pointCloud, triangles in new_contiguous_point_collection:\n",
    "            \n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        t_descriptors, t_keypoints, t_d_c = getSpinImageDescriptors(\n",
    "            pointCloud,\n",
    "            down_resolution = 5,\n",
    "            cylindrical_quantization = [4,5]\n",
    "        )\n",
    "        #print(\"Got the true descriptors\",t_descriptors.shape,t_keypoints.shape)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(\"Error getting the true descriptors of\",object_name,\"with size\",pointCloud.shape)\n",
    "\n",
    "\n",
    "    print(\"Done with\",object_name,\"in\",time.time()-t0,\"seconds.\")\n",
    "    print(\" \",pointCloud.shape,triangles.shape)\n",
    "    print(\" \",t_keypoints.shape,t_descriptors.shape)\n",
    "        \n",
    "    descriptors.append([\n",
    "        object_name,\n",
    "        t_descriptors,\n",
    "        t_keypoints,\n",
    "        t_d_c\n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment below if you want to overwrite the exisiting descriptors.\n",
    "\n",
    "\"\"\"\n",
    "    with open('descriptors/new_complete_res5_4by5_descriptors.pickle','wb') as f:\n",
    "        pickle.dump(descriptors,f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.1 Creating a synthetic set of partial spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters\n",
    "\"\"\"\n",
    "# We used a radius range of 0.25 to 5.0 in increments of 0.25\n",
    "radius_range = radius_range\n",
    "\n",
    "# We used 100 for our investigation\n",
    "samples = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE!\n",
    "There is a sample set of partial spaces in testing_samples/{}_partial_point_cloud.\n",
    "\n",
    "You may run this to produce a new set with different parameters (as above).\n",
    "If you wish to run it again, make sure you create the NNs from Step 0.1.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for radius in radius_range:\n",
    "    \n",
    "    partial_point_collection = []\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    for i in np.arange(samples):\n",
    "\n",
    "        random_object = np.random.choice(point_collection_indices)\n",
    "\n",
    "        object_name = new_contiguous_point_collection[random_object][0]\n",
    "        pointCloud = new_contiguous_point_collection[random_object][1]\n",
    "        triangles = new_contiguous_point_collection[random_object][2]\n",
    "\n",
    "        partial_pointcloud, partial_triangles, original_vertex, _v = GetPartialPointCloudOptimizedNearby(\n",
    "            np.asarray(pointCloud),\n",
    "            np.asarray(triangles),\n",
    "            radius,\n",
    "            pointCloud_index=random_object\n",
    "        )\n",
    "        \n",
    "        partial_point_collection.append([\n",
    "            [random_object, object_name, original_vertex],\n",
    "            partial_pointcloud,\n",
    "            partial_triangles            \n",
    "        ])\n",
    "        \n",
    "        with open('testing_samples/{}_partial_point_cloud.pickle'.format(radius), 'wb') as f:\n",
    "            pickle.dump(partial_point_collection,f)\n",
    "            \n",
    "        if i % 10 == (samples-1)%10:\n",
    "            print(\"Done with {} samples for radius = {:.2f} in {:.3f} seconds\".format(i,radius,time.time() - t0))\n",
    "            t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.2 Creating a synthetic set of successive partial spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Parameters\n",
    "\"\"\"\n",
    "# We used a radius range of 0.25 to 5.0 in increments of 0.25.\n",
    "radius_range = radius_range\n",
    "\n",
    "# For our work, we orignally used 50 samples with further 100 successive releases for our investigation.\n",
    "# Below are lower parameters, change as desired.\n",
    "samples = 50\n",
    "releases = 50\n",
    "\n",
    "# For demonstration purposes, we skip testing some successive samples but we still accumulate them.\n",
    "skip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE!\n",
    "There is a sample set of partial spaces in testing_samples/{}_successive_point_cloud.\n",
    "\n",
    "You may run this to produce a new set with different parameters (as above).\n",
    "If you wish to run it again, make sure you create the NNs from Step 0.1.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "for radius in radius_range:\n",
    "        \n",
    "    successive_point_collection = []\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    for i in np.arange(samples):\n",
    "\n",
    "        random_object = np.random.choice(point_collection_indices)\n",
    "\n",
    "        object_name = new_contiguous_point_collection[random_object][0]\n",
    "        pointCloud = new_contiguous_point_collection[random_object][1]\n",
    "        triangles = new_contiguous_point_collection[random_object][2]\n",
    "\n",
    "        #growing_p_pointcloud = []\n",
    "\n",
    "        growing_point_collection = []\n",
    "\n",
    "        for release in np.arange(releases):\n",
    "\n",
    "            if release == 0:\n",
    "                # For first release, as usual.\n",
    "                partial_pointcloud, partial_triangles, original_vertex, v_i = GetPartialPointCloudOptimizedNearby(\n",
    "                    np.asarray(pointCloud),\n",
    "                    np.asarray(triangles),\n",
    "                    radius,\n",
    "                    pointCloud_index=random_object,\n",
    "                    #verbose = True\n",
    "                )\n",
    "            else:\n",
    "\n",
    "                # For succeeding release, get a nearby point vertex everytime from the previous vertex\n",
    "                partial_pointcloud, partial_triangles, original_vertex, v_i = GetPartialPointCloudOptimizedNearby(\n",
    "                    np.asarray(pointCloud),\n",
    "                    np.asarray(triangles),\n",
    "                    radius = radius,\n",
    "                    vertex = previous_v_i,\n",
    "                    pointCloud_index=random_object,\n",
    "                    nearby = True,\n",
    "                    nearby_threshold=2.0\n",
    "                    #verbose = True\n",
    "                )\n",
    "                \n",
    "                #print(\"  Previous vertex\",previous_vertex[:3],previous_v_i,pointCloud[previous_v_i,:3])\n",
    "                #print(\"  New vertex\",original_vertex[:3],v_i,pointCloud[v_i,:3])                \n",
    "                \n",
    "                if LA.norm(original_vertex[:3] - previous_vertex[:3]) >= 2.0*radius:\n",
    "                    print(\"  Release: {} Warning: The distance of the succeeding vertices is {:.2f}\"\n",
    "                          .format(release,LA.norm(original_vertex[:3] - previous_vertex[:3]))\n",
    "                         )\n",
    "                    \n",
    "            previous_v_i = v_i\n",
    "            previous_vertex = original_vertex\n",
    "\n",
    "            growing_point_collection.append([\n",
    "                [random_object, object_name, original_vertex],\n",
    "                partial_pointcloud,\n",
    "                partial_triangles            \n",
    "            ])\n",
    "\n",
    "            if (release % int(releases/3) == 5) and (i % 7 == 2):\n",
    "                print(\"  Done with {} releases of sample {} for radius = {:.2f} in {:.3f} seconds\"\n",
    "                      .format(release,i,radius,time.time() - t1))\n",
    "                t1 = time.time()\n",
    "\n",
    "        successive_point_collection.append([\n",
    "            [random_object, object_name],\n",
    "            growing_point_collection\n",
    "        ])\n",
    "\n",
    "        with open('testing_samples/{}_successive_point_cloud.pickle'.format(radius), 'wb') as f:\n",
    "            pickle.dump(successive_point_collection,f)\n",
    "\n",
    "    print(\" Done with radius = {:.2f} in {:.3f} seconds\".format(radius,time.time() - t0))\n",
    "    t0 = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Extract the point cloud data from the OBJ files\n",
    " - store as numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the path to the Raw point clouds\n",
    "\n",
    "OBJ_DIR = 'point_clouds/'\n",
    "\n",
    "# Format is a tuple of (file path, file name).\n",
    "OBJ_PATHS = [ [os.path.join(OBJ_DIR, f), f]\n",
    "                    for f in os.listdir(OBJ_DIR) ]\n",
    "\n",
    "OBJ_PATHS.remove(OBJ_PATHS[0]) # remove .DS Store file\n",
    "\n",
    "# If we desire to treat the various objects within the OBJ as different spaces,\n",
    "# we store it here.\n",
    "new_per_object_collection = []\n",
    "\n",
    "# If we desire to treat the entire OBJ as a space,\n",
    "# we store it here.\n",
    "new_per_space_collection = []\n",
    "\n",
    "# This is to combine the objects within an OBJ to slightly larger objects. \n",
    "#Roughly 2.5m radius\n",
    "combining_radius = 2.5\n",
    "\n",
    "for path, filename in OBJ_PATHS:\n",
    "    \n",
    "    pointCloudFile = open(path)#'point_cloud.obj')('SpatialMesh.obj')#\n",
    "\n",
    "    pointCloudLines = pointCloudFile.readlines()\n",
    "    \n",
    "    pointCollection = [] # object_number, vertices, vertex normals, polygons\n",
    "\n",
    "    pointCollection_no_objects = [[],[],[]] \n",
    "    prev_length = 0\n",
    "    vertices_length = 1\n",
    "    \n",
    "    for line in pointCloudLines:\n",
    "\n",
    "        if line == '\\n': continue\n",
    "\n",
    "        line_items = line.split()\n",
    "\n",
    "        if line_items[0] == 'o':\n",
    "            object_item = line_items[1].split('.')\n",
    "            object_number = int(object_item[-1])\n",
    "            pointCollection.append([object_number, [], [], []])\n",
    "            vertices_length += prev_length\n",
    "\n",
    "            #print(object_number)\n",
    "\n",
    "        if line_items[0] == 'v':\n",
    "            x = line_items[1]\n",
    "            y = line_items[2]\n",
    "            z = line_items[3]\n",
    "            if len(line_items)> 4:\n",
    "                c = line_items[4]\n",
    "                if float(c) > 0.85:\n",
    "                    pointCollection[object_number-1][1].append([float(x),float(y),float(z),float(c)])\n",
    "                    pointCollection_no_objects[0].append([float(x),float(y),float(z),float(c)])\n",
    "            else:\n",
    "                pointCollection[object_number-1][1].append([float(x),float(y),float(z)])\n",
    "                pointCollection_no_objects[0].append([float(x),float(y),float(z)])\n",
    "            prev_length = len(pointCollection[object_number-1][1])\n",
    "\n",
    "\n",
    "        if line_items[0] == 'vn':\n",
    "            x = line_items[1]\n",
    "            y = line_items[2]\n",
    "            z = line_items[3]\n",
    "            pointCollection[object_number-1][2].append([float(x),float(y),float(z)])\n",
    "            pointCollection_no_objects[1].append([float(x),float(y),float(z)])\n",
    "\n",
    "        if line_items[0] == 'f':\n",
    "            p1 = int(line_items[1].split('//')[0])\n",
    "            p2 = int(line_items[2].split('//')[0])\n",
    "            p3 = int(line_items[3].split('//')[0])\n",
    "            pointCollection[object_number-1][3].append([p1-vertices_length,\n",
    "                                                        p2-vertices_length,\n",
    "                                                        p3-vertices_length])\n",
    "            pointCollection_no_objects[2].append([p1-1,p2-1,p3-1])        \n",
    "\n",
    "    pointCloudFile.close\n",
    "    print(filename)\n",
    "    #print(\" Length of point collection\",len(pointCollection[1][1]))\n",
    "    print(\" Total number of point collections\",len(pointCollection))\n",
    "    \n",
    "    #Combining objects: 1. Getting centroids\n",
    "    centroids = []\n",
    "\n",
    "    for object_name, pointCloud, _vn, triangles in pointCollection:\n",
    "        pointCloud = np.asarray(pointCloud)\n",
    "        triangles = np.asarray(triangles)#-vertices_length\n",
    "        normals = np.asarray(_vn)\n",
    "        \n",
    "        centroids.append([object_name,\n",
    "              np.mean(pointCloud[:,0]), # x-axis\n",
    "              np.mean(pointCloud[:,2]), # z-axis\n",
    "             ])\n",
    "        \n",
    "    centroids = np.asarray(centroids)\n",
    "    remaining_centroids = np.copy(centroids)\n",
    "    \n",
    "    combinedPointCollection = []\n",
    "    count = 1\n",
    "    \n",
    "    while(remaining_centroids.size!=0):\n",
    "        #print(\"  \",remaining_centroids[0,0],remaining_centroids.shape)\n",
    "        #focusing on the first remaining collection, i.e. through its centroid\n",
    "        collection = np.where(LA.norm(remaining_centroids[:,1:] - remaining_centroids[0,1:], axis = 1)<combining_radius)[0]\n",
    "        \n",
    "        #print(\"  \",collection)\n",
    "        \n",
    "        pointCloud = np.asarray(pointCollection[int(remaining_centroids[collection[0],0]-1)][1])\n",
    "        normals = np.asarray(pointCollection[int(remaining_centroids[collection[0],0])-1][2])\n",
    "        triangles = np.asarray(pointCollection[int(remaining_centroids[collection[0],0])-1][3])\n",
    "\n",
    "        for index in collection[1:]:\n",
    "            \n",
    "            n_pointCloud = np.asarray(pointCollection[int(remaining_centroids[index,0])-1][1])\n",
    "            n_normals = np.asarray(pointCollection[int(remaining_centroids[index,0])-1][2])\n",
    "            n_triangles = np.asarray(pointCollection[int(remaining_centroids[index,0])-1][3])+len(pointCloud)\n",
    "\n",
    "            pointCloud = np.concatenate((pointCloud,n_pointCloud),0)\n",
    "            normals = np.concatenate((normals,n_normals),0)\n",
    "            triangles = np.concatenate((triangles,n_triangles),0)\n",
    "        \n",
    "        new_per_object_collection.append([\n",
    "            filename+str(count),\n",
    "            pointCloud,\n",
    "            normals,\n",
    "            triangles])\n",
    "        \n",
    "        combinedPointCollection.append([\n",
    "            pointCloud,\n",
    "            normals,\n",
    "            triangles])\n",
    "            \n",
    "        count += 1\n",
    "        remaining_centroids = np.delete(remaining_centroids,collection,axis=0)\n",
    "        #print(\"   post\",remaining_centroids.shape)\n",
    "        \n",
    "    print(\" Resulting number =\",len(combinedPointCollection))\n",
    "#    new_per_object_collection.append([\n",
    "#        filename,\n",
    "#        combinedPointCollection\n",
    "#    ])\n",
    "    \n",
    "    new_per_space_collection.append([\n",
    "        filename,\n",
    "        np.asarray(pointCollection_no_objects[0]),\n",
    "        np.asarray(pointCollection_no_objects[1]),\n",
    "        np.asarray(pointCollection_no_objects[2])\n",
    "    ])\n",
    "\n",
    "print(len(new_per_object_collection),\"total objects.\")\n",
    "print(len(new_per_space_collection),\"total spaces.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combining the point position information with their normal vectors.\n",
    "\n",
    "new_contiguous_point_collection = []\n",
    "\n",
    "for object_name, pointCloud, vn, triangles in new_per_space_collection:\n",
    "    new_contiguous_point_collection.append([\n",
    "        object_name, \n",
    "        np.concatenate((pointCloud,vn),axis = 1),\n",
    "        triangles\n",
    "    ])\n",
    "    \n",
    "sample_index = np.random.choice(len(new_contiguous_point_collection))\n",
    "print(\"Sample:\",\n",
    "      new_contiguous_point_collection[sample_index][0],\n",
    "      new_contiguous_point_collection[sample_index][1].shape, # shape of the point cloud array\n",
    "      new_contiguous_point_collection[sample_index][2].shape # shape of the triangle array\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment below if you want to overwrite the exisiting point collection.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open('point_collection/new_contiguous_point_collection.pickle','wb') as f: \n",
    "    pickle.dump(new_contiguous_point_collection,f)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fig=plt.figure()#figsize=(20, 14.5))\n",
    "\n",
    "sample_name = new_contiguous_point_collection[sample_index][0]\n",
    "sample_pointCloud = new_contiguous_point_collection[sample_index][1]\n",
    "sample_triangles = new_contiguous_point_collection[sample_index][2]\n",
    "\n",
    "print(sample_name,sample_pointCloud.shape,sample_triangles.shape)\n",
    "\n",
    "ax0 = fig.add_subplot(1,1,1, projection='3d')\n",
    "ax0.set_xlabel('X')\n",
    "ax0.set_ylabel('Z')\n",
    "ax0.set_zlabel('Y')\n",
    "ax0.set_zlim(-1.5,1)\n",
    "#ax0.set_title('Dense Point Cloud')\n",
    "\n",
    "X = sample_pointCloud[:,0]\n",
    "Y = sample_pointCloud[:,1]\n",
    "Z = sample_pointCloud[:,2]\n",
    "\n",
    "ax0.set_xlim(min(X), max(X))\n",
    "ax0.set_ylim(min(-Z), max(-Z))\n",
    "\n",
    "try:\n",
    "    ax0.scatter(\n",
    "        X,-Z,Y,\n",
    "        alpha = 0.3\n",
    "    )\n",
    "\n",
    "    ax0.plot_trisurf(\n",
    "        X, -Z, Y, \n",
    "        triangles=sample_triangles,\n",
    "        cmap=plt.cm.Spectral\n",
    "        #alpha = 0.5\n",
    "    )\n",
    "\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    pass\n",
    "\n",
    "#plt.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-3d_env] *",
   "language": "python",
   "name": "conda-env-.conda-3d_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
