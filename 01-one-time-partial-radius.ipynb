{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import time\n",
    "import h5py\n",
    "import bz2\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "from mpl_toolkits.axes_grid1.axes_divider import make_axes_locatable\n",
    "\n",
    "from numpy import linalg as LA\n",
    "from scipy.spatial import Delaunay\n",
    "from sklearn.neighbors import NearestNeighbors, KDTree\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "#sys.path.insert(0, \"../\")\n",
    "from info3d import *\n",
    "from nn_matchers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the point collection and the descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('point_collection/new_contiguous_point_collection.pickle','rb') as f: \n",
    "    new_contiguous_point_collection = pickle.load(f)\n",
    "    \n",
    "with open('descriptors/new_complete_res5_4by5_descriptors.pickle','rb') as f:\n",
    "    descriptors = pickle.load(f)\n",
    "    \n",
    "with open('descriptors/new_complete_RANSAC_res5_4by5_descriptors.pickle','rb') as f:\n",
    "    ransac_descriptors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw spaces (validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = descriptors\n",
    "\n",
    "with open('sample_points.pickle','rb') as f:\n",
    "    sample_points = pickle.load(f)\n",
    "\n",
    "for radius in np.arange(0.5,2.1,0.25):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    t1 = time.time()\n",
    "\n",
    "    \n",
    "    partial_scores_pool = []\n",
    "    \n",
    "    print(\"radius =\",radius)\n",
    "    \n",
    "    for obj_, object_name, original_vertex in sample_points:\n",
    "        \n",
    "        new_partial_pointcloud = []\n",
    "        new_vX = []\n",
    "        new_vZ = []\n",
    "        \n",
    "        try:\n",
    "\n",
    "            object_, pointCloud, tri_ = new_contiguous_point_collection[int(obj_)]\n",
    "            \n",
    "            ransac_nbrs = NearestNeighbors(n_neighbors=min(20000,len(pointCloud)), algorithm='kd_tree').fit(ransac_pointCloud[:,:3])\n",
    "            \n",
    "            dist_, ind_ = ransac_nbrs.kneighbors([original_vertex[:3]])\n",
    "            pointCloud =  ransac_pointCloud[ind_[0,np.where(dist_[0,:]<=radius)[0]]]\n",
    "        except:\n",
    "            print(\"Can't get partial samples for\",obj_meta[0])\n",
    "            continue\n",
    "            \n",
    "        #if len(gen_planes) == 0: continue\n",
    "        if len(pointCloud) == 0: continue\n",
    "\n",
    "        local_keypoint_matches = []\n",
    "\n",
    "        try:\n",
    "            obj_meta, diff_ratios, diff_indexs, diff_scores, local_keypoint_matches = get_score_kdtree_lean(\n",
    "                [obj_, object_name, original_vertex], \n",
    "                pointCloud, \n",
    "                descriptors\n",
    "                #RANSAC = True,\n",
    "                # old = True\n",
    "            )\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            print(\"skipped\",object_name)\n",
    "            continue\n",
    "        \n",
    "        if len(local_keypoint_matches) == 0: \n",
    "            print(\"skipped\",object_name)\n",
    "            continue\n",
    "\n",
    "        partial_scores_pool.append([\n",
    "            [obj_, object_name, original_vertex], \n",
    "            diff_ratios,\n",
    "            diff_indexs,\n",
    "            diff_scores,\n",
    "            local_keypoint_matches\n",
    "        ])\n",
    "\n",
    "        if len(partial_scores_pool) % 33 == 1:\n",
    "            \n",
    "            partial_errors_pool = NN_matcher(partial_scores_pool)\n",
    "            print(radius,\"Error Rate:\",)\n",
    "            \n",
    "            print(\"  \",radius,\": Done with {}, in {:.3f} seconds. Error rate {:.3f}\".format(\n",
    "                len(partial_scores_pool),\n",
    "                time.time()-t1,\n",
    "                np.sum(partial_errors_pool[:,1])/len(partial_scores_pool)\n",
    "            ))\n",
    "                        \n",
    "            with bz2.BZ2File('testing_results/partial/{}_partial_scores.pickle.bz2'.format(radius), 'w') as bz2_f:\n",
    "                pickle.dump(partial_scores_pool, bz2_f)\n",
    "                \n",
    "            t1  = time.time()\n",
    "\n",
    "    print(radius,\" Total Time to match {:.3f} seconds.\".format(time.time()-t0))\n",
    "    \n",
    "\n",
    "    #print(len(partial_lengths))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = ransac_descriptors\n",
    "\n",
    "with open('sample_points.pickle','rb') as f:\n",
    "    sample_points = pickle.load(f)\n",
    "\n",
    "for radius in np.arange(2.5,3.1,0.25):\n",
    "    \n",
    "    t0 = time.time()\n",
    "    t1 = time.time()\n",
    "\n",
    "    \n",
    "    partial_scores_pool = []\n",
    "    \n",
    "    print(\"radius =\",radius)\n",
    "    \n",
    "    for obj_, object_name, original_vertex in sample_points:\n",
    "        \n",
    "        new_partial_pointcloud = []\n",
    "        new_vX = []\n",
    "        new_vZ = []\n",
    "        \n",
    "        try:\n",
    "            trial = np.random.randint(5)\n",
    "            \n",
    "            with open(\"../spatial-privacy-pointnetvlad/ransac_pc/ransac_point_collection_{}.pickle\".format(trial),'rb') as f:\n",
    "                ransac_trial_point_collection = pickle.load(f)\n",
    "\n",
    "            object_, ransac_pointCloud, tri_ = ransac_trial_point_collection[int(obj_)]\n",
    "            \n",
    "            ransac_nbrs = NearestNeighbors(n_neighbors=min(20000,len(ransac_pointCloud)), algorithm='kd_tree').fit(ransac_pointCloud[:,:3])\n",
    "            \n",
    "            dist_, ind_ = ransac_nbrs.kneighbors([original_vertex[:3]])\n",
    "            pointCloud =  ransac_pointCloud[ind_[0,np.where(dist_[0,:]<=radius)[0]]]\n",
    "        except:\n",
    "            print(\"Can't get ransac samples for\",trial,obj_meta[0],dist_.shape,ind_.shape)\n",
    "            continue\n",
    "            \n",
    "        #if len(gen_planes) == 0: continue\n",
    "        if len(pointCloud) == 0: continue\n",
    "\n",
    "        local_keypoint_matches = []\n",
    "\n",
    "        try:\n",
    "            obj_meta, diff_ratios, diff_indexs, diff_scores, local_keypoint_matches = get_score_kdtree_lean(\n",
    "                [obj_, object_name, original_vertex], \n",
    "                pointCloud, \n",
    "                descriptors\n",
    "                #RANSAC = True,\n",
    "                # old = True\n",
    "            )\n",
    "            \n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "            print(\"skipped\",object_name)\n",
    "            continue\n",
    "        \n",
    "        if len(local_keypoint_matches) == 0: \n",
    "            print(\"skipped\",object_name)\n",
    "            continue\n",
    "\n",
    "        partial_scores_pool.append([\n",
    "            [obj_, object_name, original_vertex], \n",
    "            diff_ratios,\n",
    "            diff_indexs,\n",
    "            diff_scores,\n",
    "            local_keypoint_matches\n",
    "        ])\n",
    "\n",
    "        if len(partial_scores_pool) % 33 == 1:\n",
    "            \n",
    "            partial_errors_pool = NN_matcher(partial_scores_pool)\n",
    "            print(radius,\"Error Rate:\",)\n",
    "            \n",
    "            print(\"  \",radius,\": Done with {}, in {:.3f} seconds. Error rate {:.3f}\".format(\n",
    "                len(partial_scores_pool),\n",
    "                time.time()-t1,\n",
    "                np.sum(partial_errors_pool[:,1])/len(partial_scores_pool)\n",
    "            ))\n",
    "                        \n",
    "            with bz2.BZ2File('parallel_results/{}_partial_scores_parallel_sample_points.pickle.bz2'.format(radius), 'w') as bz2_f:\n",
    "                pickle.dump(partial_scores_pool, bz2_f)\n",
    "                \n",
    "            t1  = time.time()\n",
    "\n",
    "    print(radius,\" Total Time to match {:.3f} seconds.\".format(time.time()-t0))\n",
    "    \n",
    "\n",
    "    #print(len(partial_lengths))\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-3d_env] *",
   "language": "python",
   "name": "conda-env-.conda-3d_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
